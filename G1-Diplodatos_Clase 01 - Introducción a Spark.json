{"paragraphs":[{"text":"print(s\"\"\"%html\n<center>\n    <h1><a href=\"http://diplodatos.famaf.unc.edu.ar/\">Diplomatura en Ciencia de Datos, Aprendizaje Automático y sus Aplicaciones</a></h1>\n    <h2>Curso <a href=\"https://sites.google.com/view/eleccion-optativas-diplodatos/programaci%C3%B3n-distribu%C3%ADda-sobre-grandes-vol%C3%BAmenes-de-datos\">Programación Distribuida sobre Grandes Volúmenes de Datos</a></h2>\n</center>\n\n<br>\n\n<h3 style=\"text-align:center;\"> Damián Barsotti  </h3>\n\n<h3 style=\"text-align:center;\">\n    <a href=\"http://www.famaf.unc.edu.ar\">\n    Facultad de Matemática Astronomía Física y Computación\n    </a>\n<br/>\n    <a href=\"http://www.unc.edu.ar\">\n    Universidad Nacional de Córdoba\n    </a>\n<br/>\n    <center>\n    <a href=\"http://www.famaf.unc.edu.ar\">\n    <img src=\"$baseDir/comun/logo%20UNC%20FAMAF%202016.png\" alt=\"Drawing\" style=\"width:50%;\"/>\n    </a>\n    </center>\n</h3>\n\n<p style=\"font-size:15px;\">\n    <br />\n        This work is licensed under a\n        <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.\n    <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">\n        <img alt=\"Creative Commons License\" style=\"border-width:0;vertical-align:middle;float:right\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png\" />\n    </a>\n</p>\n\"\"\")\n","user":"anonymous","dateUpdated":"2020-11-08T19:46:42-0300","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"fontSize":9,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<center>\n    <h1><a href=\"http://diplodatos.famaf.unc.edu.ar/\">Diplomatura en Ciencia de Datos, Aprendizaje Automático y sus Aplicaciones</a></h1>\n    <h2>Curso <a href=\"https://sites.google.com/view/eleccion-optativas-diplodatos/programaci%C3%B3n-distribu%C3%ADda-sobre-grandes-vol%C3%BAmenes-de-datos\">Programación Distribuida sobre Grandes Volúmenes de Datos</a></h2>\n</center>\n\n<br>\n\n<h3 style=\"text-align:center;\"> Damián Barsotti  </h3>\n\n<h3 style=\"text-align:center;\">\n    <a href=\"http://www.famaf.unc.edu.ar\">\n    Facultad de Matemática Astronomía Física y Computación\n    </a>\n<br/>\n    <a href=\"http://www.unc.edu.ar\">\n    Universidad Nacional de Córdoba\n    </a>\n<br/>\n    <center>\n    <a href=\"http://www.famaf.unc.edu.ar\">\n    <img src=\"https://bitbucket.org/bigdata_famaf/diplodatos_bigdata/raw/HEAD/clases/comun/logo%20UNC%20FAMAF%202016.png\" alt=\"Drawing\" style=\"width:50%;\"/>\n    </a>\n    </center>\n</h3>\n\n<p style=\"font-size:15px;\">\n    <br />\n        This work is licensed under a\n        <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.\n    <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">\n        <img alt=\"Creative Commons License\" style=\"border-width:0;vertical-align:middle;float:right\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png\" />\n    </a>\n</p>\n"}]},"apps":[],"jobName":"paragraph_1603494557693_-1537772390","id":"20160720-131940_474698556","dateCreated":"2020-10-23T20:09:17-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:388"},{"text":"%md\n# Introducción a Spark\n","user":"anonymous","dateUpdated":"2020-11-08T19:46:42-0300","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":false,"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Introducción a Spark</h1>\n</div>"}]},"apps":[],"jobName":"paragraph_1603494557695_-1314995553","id":"20160628-160644_98292392","dateCreated":"2020-10-23T20:09:17-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:389"},{"text":"%md\n## Características\n\n### 100x más rápido que Hadoop MapReduce en memoria.\n### 10x más rápido en disco.\n  ![](https://bitbucket.org/bigdata_famaf/diplodatos_bigdata/raw/HEAD/clases/01_intro_spark/spark_speed.png)\n","user":"anonymous","dateUpdated":"2020-11-08T19:46:42-0300","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Características</h2>\n<h3>100x más rápido que Hadoop MapReduce en memoria.</h3>\n<h3>10x más rápido en disco.</h3>\n<p><img src=\"https://bitbucket.org/bigdata_famaf/diplodatos_bigdata/raw/HEAD/clases/01_intro_spark/spark_speed.png\" /></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1603494557696_1380458947","id":"20171013-102503_1459120534","dateCreated":"2020-10-23T20:09:17-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:390"},{"text":"%md\n### Multiplataforma\n\n* Corre en Hadoop Yarn, Mesos, standalone o en la nube (AWS, Azure, ...)\n* Acceso a datos en HDFS, Cassandra, HBase, Hive, Tachyon, JDBC, etc.\n![](https://bitbucket.org/bigdata_famaf/diplodatos_bigdata/raw/HEAD/clases/01_intro_spark/spark_multi_plataforma.png)\n","user":"anonymous","dateUpdated":"2020-11-08T19:46:42-0300","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Multiplataforma</h3>\n<ul>\n  <li>Corre en Hadoop Yarn, Mesos, standalone o en la nube (AWS, Azure, &hellip;)</li>\n  <li>Acceso a datos en HDFS, Cassandra, HBase, Hive, Tachyon, JDBC, etc.<br/><img src=\"https://bitbucket.org/bigdata_famaf/diplodatos_bigdata/raw/HEAD/clases/01_intro_spark/spark_multi_plataforma.png\" /></li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1603494557696_1071611530","id":"20171013-105605_1380652694","dateCreated":"2020-10-23T20:09:17-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:391"},{"text":"%md\n### +50 empresas.\n\n### +200 desarrolladores.\n![](https://bitbucket.org/bigdata_famaf/diplodatos_bigdata/raw/HEAD/clases/01_intro_spark/spar_contribs.png)\n","user":"anonymous","dateUpdated":"2020-11-08T19:46:42-0300","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>+50 empresas.</h3>\n<h3>+200 desarrolladores.</h3>\n<p><img src=\"https://bitbucket.org/bigdata_famaf/diplodatos_bigdata/raw/HEAD/clases/01_intro_spark/spar_contribs.png\" /></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1603494557696_-1619500782","id":"20171013-112527_2104876012","dateCreated":"2020-10-23T20:09:17-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:392"},{"title":"Múltiples funcionalidades en una plataforma (Stack unificado)","text":"print(s\"\"\"%html\n<img src=\"$baseDir/01_intro_spark/unified_stack.png\" alt=\"Drawing\" style=\"width: 60%;\"/>\n\"\"\")\n","user":"anonymous","dateUpdated":"2020-11-08T19:46:42-0300","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false,"completionSupport":true,"completionKey":"TAB"},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"fontSize":9,"title":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<img src=\"https://bitbucket.org/bigdata_famaf/diplodatos_bigdata/raw/HEAD/clases/01_intro_spark/unified_stack.png\" alt=\"Drawing\" style=\"width: 60%;\"/>\n"}]},"apps":[],"jobName":"paragraph_1603494557696_-1433258811","id":"20171013-110124_1830702370","dateCreated":"2020-10-23T20:09:17-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:393"},{"text":"%md\n## Fácil de usar\n\n* Interface de programación en Scala, Java, Python y R.\n* Notebooks: Zeppelin, Jupiter, ...","user":"anonymous","dateUpdated":"2020-11-08T19:46:42-0300","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Fácil de usar</h2>\n<ul>\n  <li>Interface de programación en Scala, Java, Python y R.</li>\n  <li>Notebooks: Zeppelin, Jupiter, &hellip;</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1603494557697_578609874","id":"20171013-111851_1940139005","dateCreated":"2020-10-23T20:09:17-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:394"},{"title":"Word Count (MapReduce)","text":"%md\n```java\npublic class WordCount {\n\n\tpublic static class Map extends MapReduceBase implements Mapper<LongWritable, Text, Text, IntWritable> {\n\n\t\tprivate final static IntWritable one = new IntWritable(1);\n\n\t\tprivate Text word = new Text();\n\n\t\tpublic void map(LongWritable key, Text value, OutputCollector<Text, IntWritable> output, Reporter reporter) throws IOException {\n\n\t\t\tString line = value.toString();\n\n\t\t\tStringTokenizer tokenizer = new StringTokenizer(line);\n\n\t\t\twhile (tokenizer.hasMoreTokens()) {\n\n\t\t\t\tword.set(tokenizer.nextToken());\n\n\t\t\t\toutput.collect(word, one);\n\n\t\t\t}\n\n\t\t}\n\n\t}\n\n\tpublic static class Reduce extends MapReduceBase implements Reducer<Text, IntWritable, Text, IntWritable> {\n\n\t\tpublic void reduce(Text key, Iterator values, OutputCollector<Text, IntWritable> output, Reporter reporter) throws IOException {\n\n\t\t\tint sum = 0;\n\n\t\t\twhile (values.hasNext()) {\n\n\t\t\t\tsum += values.next().get();\n\n\t\t\t}\n\n\t\t\toutput.collect(key, new IntWritable(sum));\n\n\t\t}\n\n\t}\n\n\tpublic static void main(String[] args) throws Exception {\n\n\t\tJobConf conf = new JobConf(WordCount.class);\n\n\t\tconf.setJobName(\"wordcount\");\n\n\t\tconf.setOutputKeyClass(Text.class);\n\n\t\tconf.setOutputValueClass(IntWritable.class);\n\n\t\tconf.setMapperClass(Map.class);\n\n\t\tconf.setCombinerClass(Reduce.class);\n\n\t\tconf.setReducerClass(Reduce.class);\n\n\t\tconf.setInputFormat(TextInputFormat.class);\n\n\t\tconf.setOutputFormat(TextOutputFormat.class);\n\n\t\tFileInputFormat.setInputPaths(conf, new Path(args[0]));\n\n\t\tFileOutputFormat.setOutputPath(conf, new Path(args[1]));\n\n\t\tJobClient.runJob(conf);\n\n\t}\n\n}\n```\n\n","user":"anonymous","dateUpdated":"2020-11-08T19:46:42-0300","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":true,"results":{},"enabled":false,"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<pre><code class=\"java\">public class WordCount {\n\n\tpublic static class Map extends MapReduceBase implements Mapper&lt;LongWritable, Text, Text, IntWritable&gt; {\n\n\t\tprivate final static IntWritable one = new IntWritable(1);\n\n\t\tprivate Text word = new Text();\n\n\t\tpublic void map(LongWritable key, Text value, OutputCollector&lt;Text, IntWritable&gt; output, Reporter reporter) throws IOException {\n\n\t\t\tString line = value.toString();\n\n\t\t\tStringTokenizer tokenizer = new StringTokenizer(line);\n\n\t\t\twhile (tokenizer.hasMoreTokens()) {\n\n\t\t\t\tword.set(tokenizer.nextToken());\n\n\t\t\t\toutput.collect(word, one);\n\n\t\t\t}\n\n\t\t}\n\n\t}\n\n\tpublic static class Reduce extends MapReduceBase implements Reducer&lt;Text, IntWritable, Text, IntWritable&gt; {\n\n\t\tpublic void reduce(Text key, Iterator values, OutputCollector&lt;Text, IntWritable&gt; output, Reporter reporter) throws IOException {\n\n\t\t\tint sum = 0;\n\n\t\t\twhile (values.hasNext()) {\n\n\t\t\t\tsum += values.next().get();\n\n\t\t\t}\n\n\t\t\toutput.collect(key, new IntWritable(sum));\n\n\t\t}\n\n\t}\n\n\tpublic static void main(String[] args) throws Exception {\n\n\t\tJobConf conf = new JobConf(WordCount.class);\n\n\t\tconf.setJobName(&quot;wordcount&quot;);\n\n\t\tconf.setOutputKeyClass(Text.class);\n\n\t\tconf.setOutputValueClass(IntWritable.class);\n\n\t\tconf.setMapperClass(Map.class);\n\n\t\tconf.setCombinerClass(Reduce.class);\n\n\t\tconf.setReducerClass(Reduce.class);\n\n\t\tconf.setInputFormat(TextInputFormat.class);\n\n\t\tconf.setOutputFormat(TextOutputFormat.class);\n\n\t\tFileInputFormat.setInputPaths(conf, new Path(args[0]));\n\n\t\tFileOutputFormat.setOutputPath(conf, new Path(args[1]));\n\n\t\tJobClient.runJob(conf);\n\n\t}\n\n}\n</code></pre>\n</div>"}]},"apps":[],"jobName":"paragraph_1603494557697_-923654866","id":"20171011-151944_1744659917","dateCreated":"2020-10-23T20:09:17-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:395"},{"text":"%md\n","user":"anonymous","dateUpdated":"2020-11-08T19:46:42-0300","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1603496220423_-1831786778","id":"20201023-203700_1393557053","dateCreated":"2020-10-23T20:37:00-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:396"},{"title":"Word Count (Spark)","text":"%pyspark\n\nlines = sc.textFile(\"README.md\")\n\nwords = lines \\\n    .flatMap(lambda line: line.split(\" \")) \\\n    .filter(lambda word: word)\n\n#MapReduce\nwordCount = words \\\n    .map(lambda word: (word,1)) \\\n    .reduceByKey(lambda n,m: n+m)\n","user":"anonymous","dateUpdated":"2020-11-08T19:46:42-0300","config":{"lineNumbers":true,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/python","editorHide":false,"fontSize":14,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1603494557697_-32446204","id":"20201023-001936_119304475","dateCreated":"2020-10-23T20:09:17-0300","dateStarted":"2020-11-08T19:46:42-0300","dateFinished":"2020-11-08T19:46:43-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:397"},{"title":"Un poco de Scala","text":"%md\n\n* `lines` es un **array distribuido** de lineas de texto (`RDD[str]`).\n    - una parte del arreglo en cada **nodo del cluster**.\n\n* `lines` tiene el método `flatMap` (línea 6):\n    - `flatMap(lambda line: line.split(\" \"))` toma cada cada elemento del `RDD` (linea), lo convierte en sequencia de palabras y concatena estas secuencias:\n        - `lambda line: line.split(\" \")` es la **función** que toma una linea y la divide en una secuencia de palabras.\n        \n    - Su resultado es un array **distribuido** de palabras (`RDD[str]`).\n    \n* Al resultado de `flatMap` se aplica el método `filter` (línea 7):\n    - `filter(lambda word: word)` saca las palabras que son vacías (pueden aparecer?).\n    - `lambda word: word` es la **función** que pregunta si la palabra es vacía.\n    - `filter` devuelve un `RDD` que se almacena en `words`.\n\n* `words` tiene el método `map` (línea 11):\n    - `map(lambda word: (word,1))` agrega a cada palabra de `words` un `1`.\n    - El resultado es un **arreglo distribuido** de tuplas `RDD[(str, Int)]`.\n    \n* A este `RDD` se le aplica el método `reduceByKey` (línea 12):\n    - `reduceByKey(lambda n,m: n+m)` suma los `1`'s de las palabras iguales (la key es la palabra).\n\n","user":"anonymous","dateUpdated":"2020-11-08T19:46:43-0300","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"title":false,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<ul>\n  <li>\n    <p><code>lines</code> es un <strong>array distribuido</strong> de lineas de texto (<code>RDD[str]</code>).</p>\n    <ul>\n      <li>una parte del arreglo en cada <strong>nodo del cluster</strong>.</li>\n    </ul>\n  </li>\n  <li>\n    <p><code>lines</code> tiene el método <code>flatMap</code> (línea 6):</p>\n    <ul>\n      <li>\n        <p><code>flatMap(lambda line: line.split(&quot; &quot;))</code> toma cada cada elemento del <code>RDD</code> (linea), lo convierte en sequencia de palabras y concatena estas secuencias:</p>\n        <ul>\n          <li><code>lambda line: line.split(&quot; &quot;)</code> es la <strong>función</strong> que toma una linea y la divide en una secuencia de palabras.</li>\n        </ul>\n      </li>\n      <li>\n      <p>Su resultado es un array <strong>distribuido</strong> de palabras (<code>RDD[str]</code>).</p></li>\n    </ul>\n  </li>\n  <li>\n    <p>Al resultado de <code>flatMap</code> se aplica el método <code>filter</code> (línea 7):</p>\n    <ul>\n      <li><code>filter(lambda word: word)</code> saca las palabras que son vacías (pueden aparecer?).</li>\n      <li><code>lambda word: word</code> es la <strong>función</strong> que pregunta si la palabra es vacía.</li>\n      <li><code>filter</code> devuelve un <code>RDD</code> que se almacena en <code>words</code>.</li>\n    </ul>\n  </li>\n  <li>\n    <p><code>words</code> tiene el método <code>map</code> (línea 11):</p>\n    <ul>\n      <li><code>map(lambda word: (word,1))</code> agrega a cada palabra de <code>words</code> un <code>1</code>.</li>\n      <li>El resultado es un <strong>arreglo distribuido</strong> de tuplas <code>RDD[(str, Int)]</code>.</li>\n    </ul>\n  </li>\n  <li>\n    <p>A este <code>RDD</code> se le aplica el método <code>reduceByKey</code> (línea 12):</p>\n    <ul>\n      <li><code>reduceByKey(lambda n,m: n+m)</code> suma los <code>1</code>&rsquo;s de las palabras iguales (la key es la palabra).</li>\n    </ul>\n  </li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1603494557697_-1170337882","id":"20181010-120216_1622145406","dateCreated":"2020-10-23T20:09:17-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:398"},{"title":"Resultado Word Count Spark","text":"%pyspark\n\nresult = wordCount \\\n    .sortBy((lambda p: p[1]), ascending = False) # ordena por cantidad\n\nlocal_result = result.collect() # Traigo desde cluster\n\nfor word, count in local_result[:10]: # tomo 10\n    print(word, count) # los imprimo\n","user":"anonymous","dateUpdated":"2020-11-08T19:46:43-0300","config":{"lineNumbers":true,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":14,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"from 4\nApache 3\nZeppelin 3\nand 3\nto 3\n* 2\n### 2\nbinary 2\nPlease 2\n[User 2\n"}]},"apps":[],"jobName":"paragraph_1603494557697_-780674948","id":"20171011-153126_91229243","dateCreated":"2020-10-23T20:09:17-0300","dateStarted":"2020-11-08T19:46:43-0300","dateFinished":"2020-11-08T19:46:43-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:399"},{"title":"Run this","text":"%pyspark\n\nuiHost = sc.getConf().get(\"spark.driver.host\")#.getOrElse(\"localhost\")\nuiPort = sc.uiWebUrl.split(\":\")[-1]\n\ntextNabuco = \"\"\"%html\nEjecutar esta celda.<br>\nHacer un tunel ssh a Nabuco:<br>\nssh -vCN -L 4040:localhost:{} -l &lt;tu login&gt; nabucodonosor.ccad.unc.edu.ar<br>\ny ver Spark UI en \n<a href=\"http://{}:{}\">http://{}(host):{}(port)</a>\n\"\"\".format(uiPort,\"localhost\",\"4040\",\"localhost\",\"4040\")\n\ntextLocal = \"\"\"%html\nEjecutar esta celda y ver Spark UI en \n<a href=\"http://{}:{}\">http://{}(host):{}(port)</a>\n\"\"\".format(uiHost,uiPort,uiHost,uiPort)\n\nif uiHost == \"200.16.29.165\":\n    print(textNabuco)\nelse:\n    print(textLocal)\n","user":"anonymous","dateUpdated":"2020-11-08T19:46:43-0300","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","editorHide":true,"fontSize":15,"title":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"Ejecutar esta celda y ver Spark UI en \n<a href=\"http://192.168.0.108:4040\">http://192.168.0.108(host):4040(port)</a>\n\n"}]},"apps":[],"jobName":"paragraph_1603494557697_655569055","id":"20171010-193244_2031028749","dateCreated":"2020-10-23T20:09:17-0300","dateStarted":"2020-11-08T19:46:43-0300","dateFinished":"2020-11-08T19:46:43-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:400"},{"title":"Ejercicio 0 (word count)","text":"%md\n* Crear una celda abajo de esta (poner mouse debajo de esta celda y seleccionar \"Add Paragraph\").\n* Copiar el programa `wordcount` anterior en la misma (esta en 2 celdas).\n    - [`shift`]-[`flechas`] para seleccionar.\n    - [`ctrl`]-[`c`] para copiar.\n    - [`ctrl`]-[`v`] para pegar.\n* Modificarlo para leer todas la lineas de los archivos en `./licenses/`\n    - Ayuda: si al método `textFile` se le indica el nombre de un directorio carga todos los archivo del mismo.\n* Ejecute la celda ([`shift`]-[`enter`])\n* Ver la cantidad de tareas en SparkUI\n\n","user":"anonymous","dateUpdated":"2020-11-08T19:46:43-0300","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"title":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<ul>\n  <li>Crear una celda abajo de esta (poner mouse debajo de esta celda y seleccionar &ldquo;Add Paragraph&rdquo;).</li>\n  <li>Copiar el programa <code>wordcount</code> anterior en la misma (esta en 2 celdas).\n    <ul>\n      <li>[<code>shift</code>]-[<code>flechas</code>] para seleccionar.</li>\n      <li>[<code>ctrl</code>]-[<code>c</code>] para copiar.</li>\n      <li>[<code>ctrl</code>]-[<code>v</code>] para pegar.</li>\n    </ul>\n  </li>\n  <li>Modificarlo para leer todas la lineas de los archivos en <code>./licenses/</code>\n    <ul>\n      <li>Ayuda: si al método <code>textFile</code> se le indica el nombre de un directorio carga todos los archivo del mismo.</li>\n    </ul>\n  </li>\n  <li>Ejecute la celda ([<code>shift</code>]-[<code>enter</code>])</li>\n  <li>Ver la cantidad de tareas en SparkUI</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1603494557698_1486627780","id":"20171010-205347_2087717007","dateCreated":"2020-10-23T20:09:17-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:401"},{"text":"%pyspark\n\nlines = sc.textFile(\"./licenses\")\n\nwords = lines \\\n    .flatMap(lambda line: line.split(\" \")) \\\n    .filter(lambda word: word)\n\n#MapReduce\nwordCount = words \\\n    .map(lambda word: (word,1)) \\\n    .reduceByKey(lambda n,m: n+m)\n\n\nresult = wordCount \\\n    .sortBy((lambda p: p[1]), ascending = False) # ordena por cantidad\n\nlocal_result = result.collect() # Traigo desde cluster\n\nfor word, count in local_result[:10]: # tomo 10\n    print(word, count) # los imprimo","user":"anonymous","dateUpdated":"2020-11-08T19:46:43-0300","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"the 1098\nof 687\nto 567\nor 499\nand 473\nOR 419\nOF 324\nTHE 316\nin 289\nany 257\n"}]},"apps":[],"jobName":"paragraph_1603924909862_254783793","id":"20201028-194149_954001339","dateCreated":"2020-10-28T19:41:49-0300","dateStarted":"2020-11-08T19:46:43-0300","dateFinished":"2020-11-08T19:46:45-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:402"},{"text":"%md\n\n## Ejecución de programas en Spark\n\n* En [Zeppelin](http://zeppelin.apache.org/) (como lo hacemos ahora)\n* En `pyspark` shell (tambien interactivo)\n* Como programa autónomo\n","user":"anonymous","dateUpdated":"2020-11-08T19:46:45-0300","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Ejecución de programas en Spark</h2>\n<ul>\n  <li>En <a href=\"http://zeppelin.apache.org/\">Zeppelin</a> (como lo hacemos ahora)</li>\n  <li>En <code>pyspark</code> shell</li>\n  <li>Como programa autónomo</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1603494557698_-1761567390","id":"20171010-202757_196880209","dateCreated":"2020-10-23T20:09:17-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:403"},{"title":"pyspark shell","text":"%md\n\n* Abrir una terminal\n* Ir a la instalación Spark\n```sh\ncd ~/spark/spark-2.3.4-bin-hadoop2.7\n```\n* Arrancar el shell\n```sh\n./bin/pyspark\n```\n* Escribir en shell (después apretar `Enter`)\n```python\n>>> lines = sc.textFile(\"README.md\")\n>>> lines.first()\n```\n","user":"anonymous","dateUpdated":"2020-11-08T19:46:45-0300","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"title":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<ul>\n  <li>Abrir una terminal</li>\n  <li>\n  <p>Ir a la instalación Spark</p>\n  <pre><code class=\"sh\">cd ~/spark/spark-2.3.4-bin-hadoop2.7\n</code></pre></li>\n  <li>\n  <p>Arrancar el shell</p>\n  <pre><code class=\"sh\">./bin/pyspark\n</code></pre></li>\n  <li>\n  <p>Escribir en shell (después apretar <code>Enter</code>)</p>\n  <pre><code class=\"python\">&gt;&gt;&gt; lines = sc.textFile(&quot;README.md&quot;)\n&gt;&gt;&gt; lines.first()\n</code></pre></li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1603494557698_1296727196","id":"20171011-173126_528319238","dateCreated":"2020-10-23T20:09:17-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:404"},{"title":"Programa autónomo","text":"%md\n\n* Ir a programa\n```sh\ncd diplodatos_bigdata/prog/word_count\n```\n* Actualizar repositorio\n```sh\ngit pull\n```\n* Ver programa\n```sh\nless src/main/python/WordCount.py\n```\n  (salir con [`q`])\n","user":"anonymous","dateUpdated":"2020-11-08T19:46:45-0300","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"title":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<ul>\n  <li>\n  <p>Ir a programa</p>\n  <pre><code class=\"sh\">cd diplodatos_bigdata/prog/word_count\n</code></pre></li>\n  <li>\n  <p>Actualizar repositorio</p>\n  <pre><code class=\"sh\">git pull\n</code></pre></li>\n  <li>\n  <p>Ver programa</p>\n  <pre><code class=\"sh\">less src/main/python/WordCount.py\n</code></pre> (salir con [<code>q</code>])</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1603494557698_-287653058","id":"20171011-175259_1199949339","dateCreated":"2020-10-23T20:09:17-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:405"},{"title":"Ejecucion de programa","text":"%md\n* Ejecutar\n```sh\n~/spark/spark-2.3.4-bin-hadoop2.7/bin/spark-submit --master local[4] src/main/python/WordCount.py \\\n        ~/spark/spark-2.3.4-bin-hadoop2.7/licenses/\n```\n","user":"anonymous","dateUpdated":"2020-11-08T19:46:45-0300","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"title":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<ul>\n  <li>\n  <p>Ejecutar</p>\n  <pre><code class=\"sh\">~/spark/spark-2.3.4-bin-hadoop2.7/bin/spark-submit --master local[4] src/main/python/WordCount.py \\\n    ~/spark/spark-2.3.4-bin-hadoop2.7/licenses/\n</code></pre></li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1603494557698_1489247927","id":"20171012-165049_905215351","dateCreated":"2020-10-23T20:09:17-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:406"},{"title":"Versión Spark en Zeppelin","text":"%pyspark\n\nprint(sc.version)","user":"anonymous","dateUpdated":"2020-11-08T19:46:45-0300","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"fontSize":14,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"2.2.1\n"}]},"apps":[],"jobName":"paragraph_1603494557698_873976457","id":"20170830-114757_1684133948","dateCreated":"2020-10-23T20:09:17-0300","dateStarted":"2020-11-08T19:46:45-0300","dateFinished":"2020-11-08T19:46:45-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:407"},{"text":"%md\n\n### Principales referencias online:\n\n* [Documentación Spark](http://spark.apache.org/docs/2.2.1/)\n* [API Spark Python](http://spark.apache.org/docs/2.2.1/api/python/index.html)\n","user":"anonymous","dateUpdated":"2020-11-08T19:46:45-0300","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Principales referencias online:</h3>\n<ul>\n  <li><a href=\"http://spark.apache.org/docs/2.2.1/\">Documentación Spark</a></li>\n  <li><a href=\"http://spark.apache.org/docs/2.2.1/api/python/index.html\">API Spark Python</a></li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1603494557699_1326603513","id":"20181012-171203_1400816125","dateCreated":"2020-10-23T20:09:17-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:408"},{"text":"%md\n## Ejercicios MapReduce con Spark","user":"anonymous","dateUpdated":"2020-11-08T19:46:45-0300","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":false,"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>Ejercicios MapReduce con Spark</h2>\n</div>"}]},"apps":[],"jobName":"paragraph_1603494557699_1106250814","id":"20171016-172908_1510165702","dateCreated":"2020-10-23T20:09:17-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:409"},{"title":"Ejercicio 1","text":"%md\n\nModifique el programa *word count* siguiente para que cuente la **cantidad de apariciones de cada letra** en el archivo.\n\n* Ayuda: solo hay que modificar la linea 6\n","user":"anonymous","dateUpdated":"2020-11-08T19:46:45-0300","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"title":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Modifique el programa <em>word count</em> siguiente para que cuente la <strong>cantidad de apariciones de cada letra</strong> en el archivo.</p>\n<ul>\n  <li>Ayuda: solo hay que modificar la linea 6</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1603494557699_-1670590861","id":"20171010-202446_178633207","dateCreated":"2020-10-23T20:09:17-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:410"},{"text":"%pyspark\n\nlines = sc.textFile(\"README.md\")\n\nwords = lines \\\n    .flatMap(lambda line: line) \\\n    .filter(lambda word: word)\n\n\n#MapReduce\nwordCount = words \\\n    .map(lambda word: (word,1)) \\\n    .reduceByKey(lambda n,m: n+m)\n\nresult = wordCount \\\n    .sortBy((lambda p: p[1]), ascending = False) # ordena por cantidad\n\nlocal_result = result.collect() # Traigo desde cluster\n\nfor word, count in local_result[:50]:\n    print(word, count) # los imprimo\n","user":"anonymous","dateUpdated":"2020-11-08T19:46:45-0300","config":{"lineNumbers":true,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":14,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"e 102\n  94\nt 92\na 83\ni 73\np 61\no 59\nn 58\ns 56\nl 54\nr 54\n/ 53\nc 42\nh 36\n. 31\nu 31\n* 30\nb 29\nd 22\ng 20\nm 20\n: 18\n[ 10\n] 10\n( 10\nz 10\n) 10\n# 9\nv 9\nk 9\nS 6\nZ 6\nf 6\n> 5\nL 5\ny 5\nC 5\n< 5\nI 5\n- 5\nw 5\nA 4\nB 4\nE 4\nP 4\n, 4\nG 3\nN 2\nD 2\nU 2\n"}]},"apps":[],"jobName":"paragraph_1603494557699_1152613166","id":"20201023-001957_322623490","dateCreated":"2020-10-23T20:09:17-0300","dateStarted":"2020-11-08T19:46:45-0300","dateFinished":"2020-11-08T19:46:46-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:411"},{"title":"Ejercicio 2","text":"%md\nCada línea del archivo `~/diplodatos_bigdata/ds/links_raw.txt` contiene un url de una página web seguido de los links que posee a otras páginas web:\n```\n<url> <url link 1> <url link 2> ... <url link n>\n```\n\nBasándose en la utilización de la técnica de *MapReduce* que se mostró en el programa `word count` haga un programa en Spark que cuente la cantidad de links que apuntan a cada página.\n\n#### Ayuda\n\nA continuación está el comienzo del programa. Falta hacer el *MapReduce* y mostrar el resultado.\n","user":"anonymous","dateUpdated":"2020-11-08T19:46:46-0300","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":true,"results":{},"enabled":false,"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Cada línea del archivo <code>~/diplodatos_bigdata/ds/links_raw.txt</code> contiene un url de una página web seguido de los links que posee a otras páginas web:</p>\n<pre><code>&lt;url&gt; &lt;url link 1&gt; &lt;url link 2&gt; ... &lt;url link n&gt;\n</code></pre>\n<p>Basándose en la utilización de la técnica de <em>MapReduce</em> que se mostró en el programa <code>word count</code> haga un programa en Spark que cuente la cantidad de links que apuntan a cada página.</p>\n<h4>Ayuda</h4>\n<p>A continuación está el comienzo del programa. Falta hacer el <em>MapReduce</em> y mostrar el resultado.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1603494557699_-65061329","id":"20171011-175322_1451259292","dateCreated":"2020-10-23T20:09:17-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:412"},{"text":"%pyspark\n\nbaseDir = \"/home/leo/bigdata/diplodatos_bigdata\" # llenar con el directorio git\n\nlines = sc.textFile(baseDir + \"/ds/links_raw.txt\")\n\nlinksTo = lines \\\n    .flatMap(lambda l: l.split(\" \")[1:]) # separo los links y tomo los apuntados\n\n# Ahora linksTo tiene las paginas apuntadas\n\n# Completar los ...\n\n# MapReduce\ninvLinkCount = linksTo.map(lambda link: (link, 1)) \\\n    .reduceByKey(lambda n,m: n + m)\n\nresult = invLinkCount.sortBy((lambda p: p[1]), ascending = False)\n\nlocal_result = result.collect() # Traigo desde cluster\n\nfor word, count in local_result[:20]: # tomo 10\n    print(word, count) # los imprimo\n","user":"anonymous","dateUpdated":"2020-11-08T19:46:46-0300","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":14,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"http://www.yahoo.com/ 199\nhttp://www.ca.gov/ 169\nhttp://www.leginfo.ca.gov/calaw.html 155\nhttp://www.linkexchange.com/ 134\nhttp://www.berkeley.edu/ 126\nhttp://www.sen.ca.gov/ 123\nhttp://home.netscape.com/comprod/mirror/index.html 109\nhttp://www.assembly.ca.gov/ 99\nhttp://www.epa.gov/ 95\nhttp://www.usgs.gov/ 84\nhttp://www.house.gov/ 82\nhttp://www.fedworld.gov/ 79\nhttp://www.dot.ca.gov/ 78\nhttp://www.cnn.com/ 78\nhttp://www.exploratorium.edu/ 76\nhttp://www.berkeley.edu 73\nhttp://www.dot.ca.gov/hq/roadinfo/ 73\nhttp://www.ca.gov 73\nhttp://www.census.gov/ 71\nhttp://www.research.digital.com/SRC/virtual-tourist/California.html 70\n"}]},"apps":[],"jobName":"paragraph_1603494557699_1006607822","id":"20191121-184701_1405603118","dateCreated":"2020-10-23T20:09:17-0300","dateStarted":"2020-11-08T19:46:46-0300","dateFinished":"2020-11-08T19:46:46-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:413"},{"title":"FIN","text":"//val baseDir=\"https://git.cs.famaf.unc.edu.ar/dbarsotti/diplodatos_bigdata/raw/master/clases\"\nval baseDir=\"https://bitbucket.org/bigdata_famaf/diplodatos_bigdata/raw/HEAD/clases\"\n\nz.put(\"baseDir\", baseDir)\nprint(\"\"\"%html\n<script>\n    var heads = document.getElementsByTagName('h2');\n    var numHeads = heads.length;\n    var inner = \"\";\n    var i = 0;\n    var j = 0;\n    while (i < numHeads){\n        inner = heads[i].innerHTML;\n        if (inner.search(\".-\") != -1 ) {\n            j++;\n            heads[i].innerHTML = inner.replace(/(~|\\d+)\\.-/,\"\"+j+\".-\");\n        }\n        i++\n    }\n</script>\n\"\"\")","user":"anonymous","dateUpdated":"2020-11-08T19:46:46-0300","config":{"tableHide":true,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"fontSize":9,"title":true,"results":[{"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}}}],"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<script>\n    var heads = document.getElementsByTagName('h2');\n    var numHeads = heads.length;\n    var inner = \"\";\n    var i = 0;\n    var j = 0;\n    while (i < numHeads){\n        inner = heads[i].innerHTML;\n        if (inner.search(\".-\") != -1 ) {\n            j++;\n            heads[i].innerHTML = inner.replace(/(~|\\d+)\\.-/,\"\"+j+\".-\");\n        }\n        i++\n    }\n</script>\nbaseDir: String = https://bitbucket.org/bigdata_famaf/diplodatos_bigdata/raw/HEAD/clases\n"}]},"apps":[],"jobName":"paragraph_1603494557699_980203493","id":"20160712-175904_2058049512","dateCreated":"2020-10-23T20:09:17-0300","dateStarted":"2020-11-08T19:46:46-0300","dateFinished":"2020-11-08T19:46:46-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:414"},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1604875606472_-1776357110","id":"20201108-194646_1014275443","dateCreated":"2020-11-08T19:46:46-0300","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:415"}],"name":"Diplodatos/Clase 01 - Introducción a Spark","id":"2FQZ6Y6P5","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}