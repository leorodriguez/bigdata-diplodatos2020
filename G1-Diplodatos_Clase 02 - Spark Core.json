{"paragraphs":[{"text":"print(s\"\"\"%html\n<center>\n    <h1><a href=\"http://diplodatos.famaf.unc.edu.ar/\">Diplomatura en Ciencia de Datos, Aprendizaje Automático y sus Aplicaciones</a></h1>\n    <h2>Curso <a href=\"https://sites.google.com/view/eleccion-optativas-diplodatos/programaci%C3%B3n-distribu%C3%ADda-sobre-grandes-vol%C3%BAmenes-de-datos\">Programación Distribuida sobre Grandes Volúmenes de Datos</a></h2>\n</center>\n\n<br>\n\n<h3 style=\"text-align:center;\"> Damián Barsotti  </h3>\n\n<h3 style=\"text-align:center;\">\n    <a href=\"http://www.famaf.unc.edu.ar\">\n    Facultad de Matemática Astronomía Física y Computación\n    </a>\n<br/>\n    <a href=\"http://www.unc.edu.ar\">\n    Universidad Nacional de Córdoba\n    </a>\n<br/>\n    <center>\n    <a href=\"http://www.famaf.unc.edu.ar\">\n    <img src=\"$baseDir/comun/logo%20UNC%20FAMAF%202016.png\" alt=\"Drawing\" style=\"width:50%;\"/>\n    </a>\n    </center>\n</h3>\n\n<p style=\"font-size:15px;\">\n    <br />\n        This work is licensed under a\n        <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.\n    <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">\n        <img alt=\"Creative Commons License\" style=\"border-width:0;vertical-align:middle;float:right\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png\" />\n    </a>\n</p>\n\"\"\")\n","user":"anonymous","dateUpdated":"2020-11-08T19:47:10-0300","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"fontSize":9,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<center>\n    <h1><a href=\"http://diplodatos.famaf.unc.edu.ar/\">Diplomatura en Ciencia de Datos, Aprendizaje Automático y sus Aplicaciones</a></h1>\n    <h2>Curso <a href=\"https://sites.google.com/view/eleccion-optativas-diplodatos/programaci%C3%B3n-distribu%C3%ADda-sobre-grandes-vol%C3%BAmenes-de-datos\">Programación Distribuida sobre Grandes Volúmenes de Datos</a></h2>\n</center>\n\n<br>\n\n<h3 style=\"text-align:center;\"> Damián Barsotti  </h3>\n\n<h3 style=\"text-align:center;\">\n    <a href=\"http://www.famaf.unc.edu.ar\">\n    Facultad de Matemática Astronomía Física y Computación\n    </a>\n<br/>\n    <a href=\"http://www.unc.edu.ar\">\n    Universidad Nacional de Córdoba\n    </a>\n<br/>\n    <center>\n    <a href=\"http://www.famaf.unc.edu.ar\">\n    <img src=\"https://bitbucket.org/bigdata_famaf/diplodatos_bigdata/raw/HEAD/clases/comun/logo%20UNC%20FAMAF%202016.png\" alt=\"Drawing\" style=\"width:50%;\"/>\n    </a>\n    </center>\n</h3>\n\n<p style=\"font-size:15px;\">\n    <br />\n        This work is licensed under a\n        <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.\n    <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">\n        <img alt=\"Creative Commons License\" style=\"border-width:0;vertical-align:middle;float:right\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png\" />\n    </a>\n</p>\n"}]},"apps":[],"jobName":"paragraph_1603545804763_694163618","id":"20171010-191319_1407757246","dateCreated":"2020-10-24T10:23:24-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:2973"},{"text":"%md\n\n# Spark Core\n","user":"anonymous","dateUpdated":"2020-11-08T19:47:10-0300","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":false,"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h1>Spark Core</h1>\n</div>"}]},"apps":[],"jobName":"paragraph_1603545804766_-1406675618","id":"20171013-124120_1151991544","dateCreated":"2020-10-24T10:23:24-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2974"},{"text":"%md\n\n<br>\n### Veremos conceptos básicos  aplicables a otras librerías de [Spark](http://spark.apache.org):\n","user":"anonymous","dateUpdated":"2020-11-08T19:47:10-0300","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<br>\n<h3>Veremos conceptos básicos aplicables a otras librerías de <a href=\"http://spark.apache.org\">Spark</a>:</h3>\n</div>"}]},"apps":[],"jobName":"paragraph_1603545804766_747187497","id":"20171013-125344_626244712","dateCreated":"2020-10-24T10:23:24-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2975"},{"text":"print(s\"\"\"%html\n&nbsp;\n<img src=\"$baseDir/02_spark_core/core_stack.png\" alt=\"Drawing\" style=\"width: 100%;\"/>\n\"\"\")\n","user":"anonymous","dateUpdated":"2020-11-08T19:47:10-0300","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false,"completionSupport":true,"completionKey":"TAB"},"colWidth":6,"editorMode":"ace/mode/scala","editorHide":true,"fontSize":9,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"&nbsp;\n<img src=\"https://bitbucket.org/bigdata_famaf/diplodatos_bigdata/raw/HEAD/clases/02_spark_core/core_stack.png\" alt=\"Drawing\" style=\"width: 100%;\"/>\n"}]},"apps":[],"jobName":"paragraph_1603545804767_1923794769","id":"20171013-125319_1987010321","dateCreated":"2020-10-24T10:23:24-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2976"},{"text":"%md\n\n## ~.- Conceptos básicos\n","user":"anonymous","dateUpdated":"2020-11-08T19:47:10-0300","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>~.- Conceptos básicos</h2>\n</div>"}]},"apps":[],"jobName":"paragraph_1603545804767_-159627496","id":"20171013-125336_1933366904","dateCreated":"2020-10-24T10:23:24-0300","dateStarted":"2020-11-08T19:47:10-0300","dateFinished":"2020-11-08T19:47:10-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2977"},{"text":"%md\n\n### Driver\n\nToda aplicación Spark tiene un programa **driver**:\n\n* lanza las operaciones en el cluster,\n* contiene nuestro **programa**\n    - define datos distribuidos y les aplica operaciones.\n\n> En Zeppelin escribimos un *programa driver* que de forma interactiva ejecuta las operaciones que queremos correr.\n\n### Executors\n\nEl driver maneja y envía tareas a **executors** en los nodos del cluster (o threads en modo local).\n","user":"anonymous","dateUpdated":"2020-11-08T19:47:10-0300","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>Driver</h3>\n<p>Toda aplicación Spark tiene un programa <strong>driver</strong>:</p>\n<ul>\n  <li>lanza las operaciones en el cluster,</li>\n  <li>contiene nuestro <strong>programa</strong>\n    <ul>\n      <li>define datos distribuidos y les aplica operaciones.</li>\n    </ul>\n  </li>\n</ul>\n<blockquote>\n  <p>En Zeppelin escribimos un <em>programa driver</em> que de forma interactiva ejecuta las operaciones que queremos correr.</p>\n</blockquote>\n<h3>Executors</h3>\n<p>El driver maneja y envía tareas a <strong>executors</strong> en los nodos del cluster (o threads en modo local).</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1603545804767_1591774860","id":"20171013-130405_1538728027","dateCreated":"2020-10-24T10:23:24-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2978"},{"text":"println(s\"\"\"%html\n<img src=\"$baseDir/01_intro_spark/driver_exec.png\" alt=\"Drawing\" style=\"width: 60%;\"/>\n\"\"\")\n","user":"anonymous","dateUpdated":"2020-11-08T19:47:10-0300","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","editorHide":true,"fontSize":9,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<img src=\"https://bitbucket.org/bigdata_famaf/diplodatos_bigdata/raw/HEAD/clases/01_intro_spark/driver_exec.png\" alt=\"Drawing\" style=\"width: 60%;\"/>\n\n"}]},"apps":[],"jobName":"paragraph_1603545804767_986431687","id":"20171013-123200_262582034","dateCreated":"2020-10-24T10:23:24-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2979"},{"text":"%md\n### SparkContext\n\n","user":"anonymous","dateUpdated":"2020-11-08T19:47:10-0300","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h3>SparkContext</h3>\n</div>"}]},"apps":[],"jobName":"paragraph_1603545804768_-1375043099","id":"20171013-130511_1848331242","dateCreated":"2020-10-24T10:23:24-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2980"},{"text":"%md\n\n&nbsp;\n\n* Los programas en el driver se conectan al cluster Spark a través de un objeto `SparkContext`\n* Le dice a Spark como conectarce con el cluster (o a los distintos threads en modo local)\n    - (representa la conección al cluster) \n* En Zeppelin (y shell) está predefinida la variable `sc` de tipo `SparkContext`\n    - otros programas deben crearla con `new`\n\n![](https://bitbucket.org/bigdata_famaf/diplodatos_bigdata/raw/HEAD/clases/02_spark_core/cluster-overview.png)\n","user":"anonymous","dateUpdated":"2020-11-08T19:47:10-0300","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>&nbsp;</p>\n<ul>\n  <li>Los programas en el driver se conectan al cluster Spark a través de un objeto <code>SparkContext</code></li>\n  <li>Le dice a Spark como conectarce con el cluster (o a los distintos threads en modo local)\n    <ul>\n      <li>(representa la conección al cluster)</li>\n    </ul>\n  </li>\n  <li>En Zeppelin (y shell) está predefinida la variable <code>sc</code> de tipo <code>SparkContext</code>\n    <ul>\n      <li>otros programas deben crearla con <code>new</code></li>\n    </ul>\n  </li>\n</ul>\n<p><img src=\"https://bitbucket.org/bigdata_famaf/diplodatos_bigdata/raw/HEAD/clases/02_spark_core/cluster-overview.png\" /></p>\n</div>"}]},"apps":[],"jobName":"paragraph_1603545804768_-2139946784","id":"20171013-160636_1142900877","dateCreated":"2020-10-24T10:23:24-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2981"},{"text":"%pyspark\n\nprint(sc.defaultParallelism)\nprint(sc.master)\n","user":"anonymous","dateUpdated":"2020-11-08T19:47:10-0300","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/python","fontSize":14,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"12\nlocal[*]\n"}]},"apps":[],"jobName":"paragraph_1603545804768_-759391332","id":"20171013-131916_230493933","dateCreated":"2020-10-24T10:23:24-0300","dateStarted":"2020-11-08T19:47:10-0300","dateFinished":"2020-11-08T19:47:10-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2982"},{"title":"sc.master","text":"%md\n|master            |descripción                                               |\n|------------------|----------------------------------------------------------|\n|local             |Spark corre localmente con un solo worker (no paralelismo)|\n|local[K]          |Spark corre localmente con K threads                      |\n|spark://HOST:PORT |se conecta a un cluster Spark                             |\n|mesos://HOST:PORT |se conecta a un cluster Mesos                             |\n|yarn              |se conecta a un cluster Hadoop Yarn                       |\n|...               |...                                                       |\n\n\n\n","user":"anonymous","dateUpdated":"2020-11-08T19:47:11-0300","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"title":true,"results":{"0":{"graph":{"mode":"table","height":300,"optionOpen":false,"setting":{"table":{"tableGridState":{},"tableColumnTypeState":{"names":{"master":"string","descripción":"string"},"updated":false},"tableOptionSpecHash":"[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]","tableOptionValue":{"useFilter":false,"showPagination":false,"showAggregationFooter":false},"updated":false,"initialized":false}},"commonSetting":{}}}},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<table>\n  <thead>\n    <tr>\n      <th>master </th>\n      <th>descripción </th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>local </td>\n      <td>Spark corre localmente con un solo worker (no paralelismo)</td>\n    </tr>\n    <tr>\n      <td>local[K] </td>\n      <td>Spark corre localmente con K threads </td>\n    </tr>\n    <tr>\n      <td><a href=\"spark://HOST:PORT\">spark://HOST:PORT</a> </td>\n      <td>se conecta a un cluster Spark </td>\n    </tr>\n    <tr>\n      <td><a href=\"mesos://HOST:PORT\">mesos://HOST:PORT</a> </td>\n      <td>se conecta a un cluster Mesos </td>\n    </tr>\n    <tr>\n      <td>yarn </td>\n      <td>se conecta a un cluster Hadoop Yarn </td>\n    </tr>\n    <tr>\n      <td>&hellip; </td>\n      <td>&hellip; </td>\n    </tr>\n  </tbody>\n</table>\n</div>"}]},"apps":[],"jobName":"paragraph_1603545804768_-1941071027","id":"20191123-192357_508683745","dateCreated":"2020-10-24T10:23:24-0300","dateStarted":"2020-11-08T19:47:11-0300","dateFinished":"2020-11-08T19:47:11-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2983"},{"text":"%md\n\n## ~.- Resilient Distributed Dataset (RDD)\n","user":"anonymous","dateUpdated":"2020-11-08T19:47:11-0300","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":false,"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>~.- Resilient Distributed Dataset (RDD)</h2>\n</div>"}]},"apps":[],"jobName":"paragraph_1603545804769_861670551","id":"20171013-130245_542901367","dateCreated":"2020-10-24T10:23:24-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2984"},{"text":"%md\n* **Contenedores** de objetos **inmutables**, distribuidos en el cluster (contiene los datos)\n\n* Creados con el SparkContext `sc`.\n    - al cargar datasets a Spark\n    - por transformaciones comunes (`map`, `filter`, ...) o binarias (`union`, `intersection`, ...).\n\n* Ante fallas se reconstruyen (resilencia).\n* **Importante**: todo lo que no derive del `SparkContext` corre solo en el **driver**.\n","user":"anonymous","dateUpdated":"2020-11-08T19:47:11-0300","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":false,"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<ul>\n  <li>\n  <p><strong>Contenedores</strong> de objetos <strong>inmutables</strong>, distribuidos en el cluster (contiene los datos)</p></li>\n  <li>\n    <p>Creados con el SparkContext <code>sc</code>.</p>\n    <ul>\n      <li>al cargar datasets a Spark</li>\n      <li>por transformaciones comunes (<code>map</code>, <code>filter</code>, &hellip;) o binarias (<code>union</code>, <code>intersection</code>, &hellip;).</li>\n    </ul>\n  </li>\n  <li>\n  <p>Ante fallas se reconstruyen (resilencia).</p></li>\n  <li><strong>Importante</strong>: todo lo que no derive del <code>SparkContext</code> corre solo en el <strong>driver</strong>.</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1603545804769_-133491097","id":"20171013-161530_19251643","dateCreated":"2020-10-24T10:23:24-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2985"},{"title":"Ejemplo log analysis","text":"%pyspark\n\ninputRDD = sc.textFile(\"./logs/\") # RDD de entrada\n\n# se crea un nuevo RDD:\nerrorRDD = inputRDD.filter(lambda line: \"ERROR\" in line) \n\n# se crea otro nuevo RDD\nconfigRDD = inputRDD.filter(lambda line: \"config\" in line) \n\nerrOrConfRDD = errorRDD.union(configRDD) \n\nfor ln, l in enumerate(errOrConfRDD.collect()):\n    print(\"Linea {}:\".format(ln), l)\n","user":"anonymous","dateUpdated":"2020-11-08T19:47:11-0300","config":{"lineNumbers":true,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/python","fontSize":14,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Linea 0:  WARN [2020-11-08 12:10:54,455] ({pool-2-thread-21} NotebookServer.java[afterStatusChange]:2316) - Job 20201108-120604_1359724646 is finished, status: ERROR, exception: null, result: %text Fail to execute line 1: flightsDF.groupBy(\"Origin\") \\\nLinea 1:  WARN [2020-11-08 12:14:27,366] ({pool-2-thread-22} NotebookServer.java[afterStatusChange]:2316) - Job 20201108-120604_1359724646 is finished, status: ERROR, exception: null, result: %text Traceback (most recent call last):\nLinea 2:  WARN [2020-11-08 12:23:57,123] ({pool-2-thread-24} NotebookServer.java[afterStatusChange]:2316) - Job 20201108-122341_1173897026 is finished, status: ERROR, exception: null, result: %text \nLinea 3:  WARN [2020-11-08 12:25:49,747] ({pool-2-thread-26} NotebookServer.java[afterStatusChange]:2316) - Job 20171026-092248_827292703 is finished, status: ERROR, exception: null, result: %text Table or view not found: flightsPermTbl; line 1 pos 14\nLinea 4:  WARN [2020-11-08 12:42:53,725] ({pool-2-thread-5} NotebookServer.java[afterStatusChange]:2316) - Job 20201108-123839_1200378639 is finished, status: ERROR, exception: null, result: %text Fail to execute line 6: flAirportDF.count()\nLinea 5:  WARN [2020-11-08 12:48:50,676] ({pool-2-thread-17} NotebookServer.java[afterStatusChange]:2316) - Job 20201108-123543_1838428750 is finished, status: ERROR, exception: null, result: %text Traceback (most recent call last):\nLinea 6:  WARN [2020-11-08 12:49:03,187] ({pool-2-thread-33} NotebookServer.java[afterStatusChange]:2316) - Job 20201108-123543_1838428750 is finished, status: ERROR, exception: null, result: %text Fail to execute line 4: z.show(airportsDF.map(lambda d: d[\"iata\"]))\nLinea 7:  WARN [2020-11-08 12:49:57,919] ({pool-2-thread-2} NotebookServer.java[afterStatusChange]:2316) - Job 20201108-123543_1838428750 is finished, status: ERROR, exception: null, result: %text Traceback (most recent call last):\nLinea 8:  WARN [2020-11-08 12:50:12,459] ({pool-2-thread-34} NotebookServer.java[afterStatusChange]:2316) - Job 20201108-123543_1838428750 is finished, status: ERROR, exception: null, result: %text Fail to execute line 4: z.show(airportsDF.map(lambda d: d(\"iata\")))\nLinea 9:  WARN [2020-11-08 12:52:05,486] ({pool-2-thread-18} NotebookServer.java[afterStatusChange]:2316) - Job 20201108-123543_1838428750 is finished, status: ERROR, exception: null, result: %text Fail to execute line 4: z.show(airportsDF.select(\"idata\"))\nLinea 10:  WARN [2020-11-08 12:55:14,698] ({pool-2-thread-6} NotebookServer.java[afterStatusChange]:2316) - Job 20201108-123543_1838428750 is finished, status: ERROR, exception: null, result: %text Fail to execute line 4: z.show(airportsDF.select(\"iata\").filter(\"W\" in col(\"iata\")))\nLinea 11:  WARN [2020-11-08 13:07:10,099] ({pool-2-thread-39} NotebookServer.java[afterStatusChange]:2316) - Job 20201108-123839_1200378639 is finished, status: ERROR, exception: null, result: %text Fail to execute line 5: z.show(aiportNames)\nLinea 12:  WARN [2020-11-08 14:00:26,660] ({pool-2-thread-21} NotebookServer.java[afterStatusChange]:2316) - Job 20201108-123839_1200378639 is finished, status: ERROR, exception: null, result: %table iata\tairport\nLinea 13:  WARN [2020-11-08 14:07:58,907] ({pool-2-thread-22} NotebookServer.java[afterStatusChange]:2316) - Job 20201108-123839_1200378639 is finished, status: ERROR, exception: null, result: %text Traceback (most recent call last):\nLinea 14:  WARN [2020-11-08 14:11:30,098] ({pool-2-thread-45} NotebookServer.java[afterStatusChange]:2316) - Job 20201108-123839_1200378639 is finished, status: ERROR, exception: null, result: %text Fail to execute line 14:             .join(airportData, col(\"iata\") == col(\"DestCode\")) \\\nLinea 15:  WARN [2020-11-08 14:12:12,852] ({pool-2-thread-7} NotebookServer.java[afterStatusChange]:2316) - Job 20201108-123839_1200378639 is finished, status: ERROR, exception: null, result: %text Fail to execute line 14:             .join(airportData, col(\"iata\") == col(\"DestCode\")) \\\nLinea 16:  WARN [2020-11-08 14:49:02,289] ({pool-2-thread-54} NotebookServer.java[afterStatusChange]:2316) - Job 20171030-192143_1165730818 is finished, status: ERROR, exception: null, result: %text Traceback (most recent call last):\nLinea 17:  WARN [2020-11-08 14:59:19,496] ({pool-2-thread-16} NotebookServer.java[afterStatusChange]:2316) - Job 20171030-202050_1263604133 is finished, status: ERROR, exception: null, result: %text Fail to execute line 14:                     .select(\"*\", ith(\"probability\",lit(1-posF)).alias(\"prob\"))\nLinea 18:  WARN [2020-11-08 15:00:38,965] ({pool-2-thread-60} NotebookServer.java[afterStatusChange]:2316) - Job 20201029-105351_101361532 is finished, status: ERROR, exception: null, result: %text Fail to execute line 6:     degree=...) # llenar (ver documentacion)\nLinea 19:  WARN [2020-11-08 15:03:24,588] ({pool-2-thread-31} NotebookServer.java[afterStatusChange]:2316) - Job 20201029-105351_101361532 is finished, status: ERROR, exception: null, result: %text Fail to execute line 15: lrModelPoly = lrEstimatorPoly.fit(...) # llenar\nLinea 20:  WARN [2020-11-08 15:04:42,098] ({pool-2-thread-61} NotebookServer.java[afterStatusChange]:2316) - Job 20201029-105351_101361532 is finished, status: ERROR, exception: null, result: %text Fail to execute line 20:                             .select(\"*\", ith(\"probability\",lit(1-posF)).alias(\"prob\"))\nLinea 21:  WARN [2020-11-08 15:09:07,809] ({pool-2-thread-17} NotebookServer.java[afterStatusChange]:2316) - Job 20171030-040455_976462143 is finished, status: ERROR, exception: null, result: %text Fail to execute line 1: plot_classification(testFeaturized, surfaceTable=gridPredictionLRPoly, prob=True)\nLinea 22:  WARN [2020-11-08 15:11:06,148] ({pool-2-thread-64} NotebookServer.java[afterStatusChange]:2316) - Job 20171030-040455_976462143 is finished, status: ERROR, exception: null, result: %text Fail to execute line 1: plot_classification(testFeaturized, surfaceTable=gridPredictionLRPoly, prob=True)\nLinea 23:  WARN [2020-11-08 16:07:31,552] ({pool-2-thread-81} NotebookServer.java[afterStatusChange]:2316) - Job 20201108-160657_875166737 is finished, status: ERROR, exception: null, result: %text Fail to execute line 2: gridPredictionDT = pipeline.transform(grid)\nLinea 24:  WARN [2020-11-08 16:07:45,390] ({pool-2-thread-4} NotebookServer.java[afterStatusChange]:2316) - Job 20201108-160657_875166737 is finished, status: ERROR, exception: null, result: %text Fail to execute line 2: gridPredictionDT = pipModel.transform(grid)\nLinea 25:  WARN [2020-11-08 16:09:34,901] ({pool-2-thread-42} NotebookServer.java[afterStatusChange]:2316) - Job 20201108-160657_875166737 is finished, status: ERROR, exception: null, result: %text Fail to execute line 2: gridPredictionDT = pipModel.transform(grid)\nLinea 26:  WARN [2020-11-08 17:02:41,299] ({pool-2-thread-51} NotebookServer.java[afterStatusChange]:2316) - Job 20171101-170622_751215254 is finished, status: ERROR, exception: null, result: %text Traceback (most recent call last):\nLinea 27:  WARN [2020-11-08 17:02:50,518] ({pool-2-thread-14} NotebookServer.java[afterStatusChange]:2316) - Job 20171101-170622_751215254 is finished, status: ERROR, exception: null, result: %text Traceback (most recent call last):\nLinea 28:  WARN [2020-11-08 17:03:06,073] ({pool-2-thread-52} NotebookServer.java[afterStatusChange]:2316) - Job 20171101-170622_751215254 is finished, status: ERROR, exception: null, result: %text Traceback (most recent call last):\nLinea 29:  WARN [2020-11-08 17:03:25,341] ({pool-2-thread-27} NotebookServer.java[afterStatusChange]:2316) - Job 20171101-170622_751215254 is finished, status: ERROR, exception: null, result: %text Traceback (most recent call last):\nLinea 30:  WARN [2020-11-08 17:03:34,868] ({pool-2-thread-53} NotebookServer.java[afterStatusChange]:2316) - Job 20171101-170622_751215254 is finished, status: ERROR, exception: null, result: %text Fail to execute line 5: topInfluencers.select(\"*\", col(\"id\").alias(\"Label\")).coalesce(1) \\\nLinea 31:  WARN [2020-11-08 17:04:58,875] ({pool-2-thread-8} NotebookServer.java[afterStatusChange]:2316) - Job 20171101-170622_751215254 is finished, status: ERROR, exception: null, result: %text Fail to execute line 5: topInfluencers.select(\"*\", col(\"id\").alias(\"Label\")).coalesce(1) \\\nLinea 32:  WARN [2020-11-08 17:05:46,979] ({pool-2-thread-54} NotebookServer.java[afterStatusChange]:2316) - Job 20171101-170622_751215254 is finished, status: ERROR, exception: null, result: %text Fail to execute line 5: topInfluencers.select(\"*\", col(\"id\").alias(\"Label\")).coalesce(1) \\\nLinea 33:  WARN [2020-11-08 17:07:55,985] ({pool-2-thread-28} NotebookServer.java[afterStatusChange]:2316) - Job 20171101-170622_751215254 is finished, status: ERROR, exception: null, result: %text Fail to execute line 5: topInfluencers.select(\"*\", col(\"id\").alias(\"Label\")).coalesce(1) \\\nLinea 34:  WARN [2020-11-08 18:51:31,848] ({pool-2-thread-2} NotebookServer.java[afterStatusChange]:2316) - Job 20171101-170622_751215254 is finished, status: ERROR, exception: null, result: %text Fail to execute line 2: topInfluencers = collectiveInfluence.limit(5) # solo los 5 primeros\nLinea 35:  WARN [2020-11-08 18:52:39,499] ({pool-2-thread-9} NotebookServer.java[afterStatusChange]:2316) - Job 20171101-170622_751215254 is finished, status: ERROR, exception: null, result: %text Py4JJavaError: An error occurred while calling o313.find.\nLinea 36:  WARN [2020-11-08 18:55:09,688] ({pool-2-thread-15} NotebookServer.java[afterStatusChange]:2316) - Job 20171101-170622_751215254 is finished, status: ERROR, exception: null, result: %text Py4JJavaError: An error occurred while calling o346.find.\nLinea 37:  WARN [2020-11-08 19:00:00,436] ({pool-2-thread-6} NotebookServer.java[afterStatusChange]:2316) - Job 20201108-185551_1769688009 is finished, status: ERROR, exception: null, result: %text Py4JJavaError: An error occurred while calling o173.find.\nLinea 38:  WARN [2020-11-08 19:00:21,509] ({pool-2-thread-17} NotebookServer.java[afterStatusChange]:2316) - Job 20201108-185551_1769688009 is finished, status: ERROR, exception: null, result: %text Py4JJavaError: An error occurred while calling o173.find.\nLinea 39:  WARN [2020-11-08 19:03:21,164] ({pool-2-thread-4} NotebookServer.java[afterStatusChange]:2316) - Job 20201108-185551_1769688009 is finished, status: ERROR, exception: null, result: %text Traceback (most recent call last):\nLinea 40:  WARN [2020-11-08 19:05:31,098] ({pool-2-thread-20} NotebookServer.java[afterStatusChange]:2316) - Job 20201108-185551_1769688009 is finished, status: ERROR, exception: null, result: %text Fail to execute line 2:             .filter(col(\"a\") != col(\"b\") & col(\"b\") != col(\"c\") & col(\"c\") != col(\"a\"))\nLinea 41:  WARN [2020-11-08 19:16:55,574] ({pool-2-thread-24} NotebookServer.java[afterStatusChange]:2316) - Job 20201108-185551_1769688009 is finished, status: ERROR, exception: null, result: %text Fail to execute line 5: nodes_a = triangles.select(col(\"a\")(0))\nLinea 42:  WARN [2020-11-08 19:17:45,513] ({pool-2-thread-8} NotebookServer.java[afterStatusChange]:2316) - Job 20201108-185551_1769688009 is finished, status: ERROR, exception: null, result: %text Fail to execute line 5: nodes_a = triangles.select(col(\"a\")-getItem(0))\nLinea 43:  WARN [2020-11-08 19:19:29,665] ({pool-2-thread-25} NotebookServer.java[afterStatusChange]:2316) - Job 20201108-185551_1769688009 is finished, status: ERROR, exception: null, result: %text Traceback (most recent call last):\nLinea 44:  WARN [2020-11-08 19:19:39,136] ({pool-2-thread-14} NotebookServer.java[afterStatusChange]:2316) - Job 20201108-185551_1769688009 is finished, status: ERROR, exception: null, result: %text Fail to execute line 8: nodes_a = triangles.select(expr(\"a[0]\"))\nLinea 45:  WARN [2020-11-08 19:20:25,666] ({pool-2-thread-26} NotebookServer.java[afterStatusChange]:2316) - Job 20201108-185551_1769688009 is finished, status: ERROR, exception: null, result: %text Fail to execute line 8: nodes_a = triangles.select(expr(\"a[0]\"))\nLinea 46:  WARN [2020-11-08 19:21:15,014] ({pool-2-thread-9} NotebookServer.java[afterStatusChange]:2316) - Job 20201108-185551_1769688009 is finished, status: ERROR, exception: null, result: %text root\nLinea 47:  WARN [2020-11-08 19:26:17,723] ({pool-2-thread-29} NotebookServer.java[afterStatusChange]:2316) - Job 20201108-185551_1769688009 is finished, status: ERROR, exception: null, result: %text root\nLinea 48:  WARN [2020-10-23 21:11:56,175] ({pool-2-thread-2} NotebookServer.java[afterStatusChange]:2316) - Job 20201023-001936_119304475 is finished, status: ERROR, exception: null, result: %text Matplotlib is building the font cache; this may take a moment.\nLinea 49:  WARN [2020-10-28 19:43:41,176] ({pool-2-thread-3} NotebookServer.java[afterStatusChange]:2316) - Job 20201028-194149_954001339 is finished, status: ERROR, exception: null, result: %text Py4JJavaError: An error occurred while calling o107.partitions.\nLinea 50:  WARN [2020-10-28 20:18:21,912] ({pool-2-thread-5} NotebookServer.java[afterStatusChange]:2316) - Job 20171013-175507_696892344 is finished, status: ERROR, exception: null, result: %text Traceback (most recent call last):\nLinea 51:  WARN [2020-10-28 20:23:23,717] ({pool-2-thread-6} NotebookServer.java[afterStatusChange]:2316) - Job 20191123-214023_2104486544 is finished, status: ERROR, exception: null, result: %text Fail to execute line 9: linesError = inesStrip.filter(lambda l: l.startswith(\"ERROR\")) # Completar\nLinea 52:  WARN [2020-10-28 20:35:26,241] ({pool-2-thread-3} NotebookServer.java[afterStatusChange]:2316) - Job 20191124-133441_1910745321 is finished, status: ERROR, exception: null, result: %text Fail to execute line 7: print(parser.take(4))\nLinea 53:  WARN [2020-10-28 20:36:14,348] ({pool-2-thread-8} NotebookServer.java[afterStatusChange]:2316) - Job 20191124-133441_1910745321 is finished, status: ERROR, exception: null, result: %text Fail to execute line 7: print(parsed.take(4).map(lambda l: l[21]))\nLinea 54:  WARN [2020-10-28 20:40:02,538] ({pool-2-thread-16} NotebookServer.java[afterStatusChange]:2316) - Job 20201028-203729_875715534 is finished, status: ERROR, exception: null, result: %text Fail to execute line 4: max_time = parser.map(lambda: l[13])\nLinea 55:  WARN [2020-10-28 20:40:09,256] ({pool-2-thread-9} NotebookServer.java[afterStatusChange]:2316) - Job 20201028-203729_875715534 is finished, status: ERROR, exception: null, result: %text Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.\nLinea 56:  WARN [2020-10-28 20:41:21,492] ({pool-2-thread-2} NotebookServer.java[afterStatusChange]:2316) - Job 20201028-203729_875715534 is finished, status: ERROR, exception: null, result: %text Fail to execute line 4: max_time = parsed.tail.map(lambda l: l[13])\nLinea 57:  WARN [2020-10-28 20:42:18,683] ({pool-2-thread-18} NotebookServer.java[afterStatusChange]:2316) - Job 20201028-203729_875715534 is finished, status: ERROR, exception: null, result: %text Fail to execute line 4: max_time = parsed.map(lambda l: l[13]).drop(1)\nLinea 58:  WARN [2020-10-28 20:45:04,727] ({pool-2-thread-11} NotebookServer.java[afterStatusChange]:2316) - Job 20201028-203729_875715534 is finished, status: ERROR, exception: null, result: %text Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\nLinea 59:  WARN [2020-10-28 20:45:14,905] ({pool-2-thread-21} NotebookServer.java[afterStatusChange]:2316) - Job 20201028-203729_875715534 is finished, status: ERROR, exception: null, result: %text Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\nLinea 60:  WARN [2020-10-28 21:26:43,105] ({pool-2-thread-5} NotebookServer.java[afterStatusChange]:2316) - Job 20201023-123443_40437356 is finished, status: ERROR, exception: null, result: %text Fail to execute line 5: spark.sql(\"create table gen_prom as SELECT gender, avg(...) AS age_avg FROM users GROUP BY ...\")\nLinea 61:  WARN [2020-10-28 21:27:43,263] ({pool-2-thread-16} NotebookServer.java[afterStatusChange]:2316) - Job 20191128-172216_319223823 is finished, status: ERROR, exception: null, result: %text Traceback (most recent call last):\nLinea 62:  WARN [2020-10-28 21:28:13,225] ({pool-2-thread-31} NotebookServer.java[afterStatusChange]:2316) - Job 20191128-172216_319223823 is finished, status: ERROR, exception: null, result: %text Fail to execute line 6:             .groupBy(age).agg(avg(age).alias(\"age_avg\"))\nLinea 63: ERROR [2020-10-28 20:40:09,235] ({Executor task launch worker for task 521} Logging.scala[logError]:91) - Exception in task 0.0 in stage 85.0 (TID 521)\nLinea 64: ERROR [2020-10-28 20:40:09,245] ({task-result-getter-3} Logging.scala[logError]:70) - Task 0 in stage 85.0 failed 1 times; aborting job\nLinea 65: ERROR [2020-10-28 20:45:04,718] ({Executor task launch worker for task 528} Logging.scala[logError]:91) - Exception in task 1.0 in stage 91.0 (TID 528)\nLinea 66: ERROR [2020-10-28 20:45:04,718] ({Executor task launch worker for task 527} Logging.scala[logError]:91) - Exception in task 0.0 in stage 91.0 (TID 527)\nLinea 67: ERROR [2020-10-28 20:45:04,719] ({task-result-getter-1} Logging.scala[logError]:70) - Task 1 in stage 91.0 failed 1 times; aborting job\nLinea 68: ERROR [2020-10-28 20:45:14,894] ({Executor task launch worker for task 529} Logging.scala[logError]:91) - Exception in task 0.0 in stage 92.0 (TID 529)\nLinea 69: ERROR [2020-10-28 20:45:14,895] ({task-result-getter-3} Logging.scala[logError]:70) - Task 0 in stage 92.0 failed 1 times; aborting job\nLinea 70: ERROR [2020-11-08 12:23:57,118] ({pool-2-thread-25} SparkSqlInterpreter.java[interpret]:127) - Invocation target exception\nLinea 71: ERROR [2020-11-08 12:25:49,745] ({pool-2-thread-27} SparkSqlInterpreter.java[interpret]:127) - Invocation target exception\nLinea 72: ERROR [2020-11-08 14:00:51,260] ({SparkListenerBus} Logging.scala[logError]:91) - Listener  threw an exception\nLinea 73: ERROR [2020-11-08 14:00:51,284] ({SparkListenerBus} Logging.scala[logError]:91) - Listener  threw an exception\nLinea 74: ERROR [2020-11-08 14:03:13,511] ({SparkListenerBus} Logging.scala[logError]:91) - Listener  threw an exception\nLinea 75: ERROR [2020-11-08 14:03:13,525] ({SparkListenerBus} Logging.scala[logError]:91) - Listener  threw an exception\nLinea 76: ERROR [2020-11-08 14:04:28,858] ({SparkListenerBus} Logging.scala[logError]:91) - Listener  threw an exception\nLinea 77: ERROR [2020-11-08 14:04:28,871] ({SparkListenerBus} Logging.scala[logError]:91) - Listener  threw an exception\nLinea 78: ERROR [2020-11-08 14:08:12,443] ({SparkListenerBus} Logging.scala[logError]:91) - Listener  threw an exception\nLinea 79: ERROR [2020-11-08 14:08:12,464] ({SparkListenerBus} Logging.scala[logError]:91) - Listener  threw an exception\nLinea 80: ERROR [2020-11-08 14:59:00,480] ({Thread-17} Logger.scala[error]:27) - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\nLinea 81: ERROR [2020-11-08 15:04:41,609] ({Thread-17} Logger.scala[error]:27) - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\nLinea 82: ERROR [2020-11-08 15:08:55,485] ({Thread-17} Logger.scala[error]:27) - Failure! Resetting history: breeze.optimize.FirstOrderException: Line search failed\nLinea 83:  INFO [2020-10-24 10:19:27,101] ({main} ZeppelinConfiguration.java[create]:121) - Load configuration from file:/home/leo/bigdata/spark/zeppelin-0.8.2-bin-all/conf/zeppelin-site.xml\nLinea 84:  WARN [2020-10-24 10:19:28,603] ({main} ZeppelinConfiguration.java[getConfigFSDir]:527) - zeppelin.config.fs.dir is not specified, fall back to local conf directory zeppelin.conf.dir\nLinea 85:  WARN [2020-10-24 10:19:28,604] ({main} ZeppelinConfiguration.java[getConfigFSDir]:527) - zeppelin.config.fs.dir is not specified, fall back to local conf directory zeppelin.conf.dir\nLinea 86:  WARN [2020-10-24 10:19:29,506] ({main} NotebookAuthorization.java[getInstance]:86) - Notebook authorization module was called without initialization, initializing with default configuration\nLinea 87:  INFO [2020-10-24 10:39:40,477] ({main} ZeppelinConfiguration.java[create]:121) - Load configuration from file:/home/leo/bigdata/spark/zeppelin-0.8.2-bin-all/conf/zeppelin-site.xml\nLinea 88:  WARN [2020-10-24 10:39:41,808] ({main} ZeppelinConfiguration.java[getConfigFSDir]:527) - zeppelin.config.fs.dir is not specified, fall back to local conf directory zeppelin.conf.dir\nLinea 89:  WARN [2020-10-24 10:39:41,809] ({main} ZeppelinConfiguration.java[getConfigFSDir]:527) - zeppelin.config.fs.dir is not specified, fall back to local conf directory zeppelin.conf.dir\nLinea 90:  WARN [2020-10-24 10:39:42,233] ({main} NotebookAuthorization.java[getInstance]:86) - Notebook authorization module was called without initialization, initializing with default configuration\nLinea 91:  INFO [2020-10-24 11:18:05,603] ({main} ZeppelinConfiguration.java[create]:121) - Load configuration from file:/home/leo/bigdata/spark/zeppelin-0.8.2-bin-all/conf/zeppelin-site.xml\nLinea 92:  WARN [2020-10-24 11:18:07,134] ({main} ZeppelinConfiguration.java[getConfigFSDir]:527) - zeppelin.config.fs.dir is not specified, fall back to local conf directory zeppelin.conf.dir\nLinea 93:  WARN [2020-10-24 11:18:07,134] ({main} ZeppelinConfiguration.java[getConfigFSDir]:527) - zeppelin.config.fs.dir is not specified, fall back to local conf directory zeppelin.conf.dir\nLinea 94:  WARN [2020-10-24 11:18:07,614] ({main} NotebookAuthorization.java[getInstance]:86) - Notebook authorization module was called without initialization, initializing with default configuration\nLinea 95:  WARN [2020-10-28 20:50:37,077] ({pool-1-thread-1} ZeppelinConfiguration.java[create]:117) - Failed to load configuration, proceeding with a default\nLinea 96:  INFO [2020-10-30 18:11:05,959] ({main} ZeppelinConfiguration.java[create]:121) - Load configuration from file:/home/leo/bigdata/spark/zeppelin-0.8.2-bin-all/conf/zeppelin-site.xml\nLinea 97:  WARN [2020-10-30 18:11:07,689] ({main} ZeppelinConfiguration.java[getConfigFSDir]:527) - zeppelin.config.fs.dir is not specified, fall back to local conf directory zeppelin.conf.dir\nLinea 98:  WARN [2020-10-30 18:11:07,691] ({main} ZeppelinConfiguration.java[getConfigFSDir]:527) - zeppelin.config.fs.dir is not specified, fall back to local conf directory zeppelin.conf.dir\nLinea 99:  WARN [2020-10-30 18:11:08,402] ({main} NotebookAuthorization.java[getInstance]:86) - Notebook authorization module was called without initialization, initializing with default configuration\nLinea 100:  INFO [2020-10-30 18:12:30,523] ({main} ZeppelinConfiguration.java[create]:121) - Load configuration from file:/home/leo/bigdata/spark/zeppelin-0.8.2-bin-all/conf/zeppelin-site.xml\nLinea 101:  WARN [2020-10-30 18:12:31,941] ({main} ZeppelinConfiguration.java[getConfigFSDir]:527) - zeppelin.config.fs.dir is not specified, fall back to local conf directory zeppelin.conf.dir\nLinea 102:  WARN [2020-10-30 18:12:31,942] ({main} ZeppelinConfiguration.java[getConfigFSDir]:527) - zeppelin.config.fs.dir is not specified, fall back to local conf directory zeppelin.conf.dir\nLinea 103:  WARN [2020-10-30 18:12:32,386] ({main} NotebookAuthorization.java[getInstance]:86) - Notebook authorization module was called without initialization, initializing with default configuration\nLinea 104:  INFO [2020-11-08 11:28:25,815] ({main} ZeppelinConfiguration.java[create]:121) - Load configuration from file:/home/leo/bigdata/spark/zeppelin-0.8.2-bin-all/conf/zeppelin-site.xml\nLinea 105:  WARN [2020-11-08 11:28:27,512] ({main} ZeppelinConfiguration.java[getConfigFSDir]:527) - zeppelin.config.fs.dir is not specified, fall back to local conf directory zeppelin.conf.dir\nLinea 106:  WARN [2020-11-08 11:28:27,512] ({main} ZeppelinConfiguration.java[getConfigFSDir]:527) - zeppelin.config.fs.dir is not specified, fall back to local conf directory zeppelin.conf.dir\nLinea 107:  WARN [2020-11-08 11:28:28,251] ({main} NotebookAuthorization.java[getInstance]:86) - Notebook authorization module was called without initialization, initializing with default configuration\nLinea 108:  INFO [2020-11-08 18:03:02,246] ({main} ZeppelinConfiguration.java[create]:121) - Load configuration from file:/home/leo/bigdata/spark/zeppelin-0.8.2-bin-all/conf/zeppelin-site.xml\nLinea 109:  WARN [2020-11-08 18:03:03,567] ({main} ZeppelinConfiguration.java[getConfigFSDir]:527) - zeppelin.config.fs.dir is not specified, fall back to local conf directory zeppelin.conf.dir\nLinea 110:  WARN [2020-11-08 18:03:03,567] ({main} ZeppelinConfiguration.java[getConfigFSDir]:527) - zeppelin.config.fs.dir is not specified, fall back to local conf directory zeppelin.conf.dir\nLinea 111:  WARN [2020-11-08 18:03:03,981] ({main} NotebookAuthorization.java[getInstance]:86) - Notebook authorization module was called without initialization, initializing with default configuration\nLinea 112:  INFO [2020-10-23 18:11:08,956] ({main} ZeppelinConfiguration.java[create]:121) - Load configuration from file:/home/leo/bigdata/spark/zeppelin-0.8.2-bin-all/conf/zeppelin-site.xml\nLinea 113:  WARN [2020-10-23 18:11:11,741] ({main} ZeppelinConfiguration.java[getConfigFSDir]:527) - zeppelin.config.fs.dir is not specified, fall back to local conf directory zeppelin.conf.dir\nLinea 114:  WARN [2020-10-23 18:11:11,746] ({main} ZeppelinConfiguration.java[getConfigFSDir]:527) - zeppelin.config.fs.dir is not specified, fall back to local conf directory zeppelin.conf.dir\nLinea 115:  WARN [2020-10-23 18:11:13,228] ({main} NotebookAuthorization.java[getInstance]:86) - Notebook authorization module was called without initialization, initializing with default configuration\nLinea 116:  INFO [2020-10-23 21:13:15,323] ({main} ZeppelinConfiguration.java[create]:121) - Load configuration from file:/home/leo/bigdata/spark/zeppelin-0.8.2-bin-all/conf/zeppelin-site.xml\nLinea 117:  WARN [2020-10-23 21:13:16,735] ({main} ZeppelinConfiguration.java[getConfigFSDir]:527) - zeppelin.config.fs.dir is not specified, fall back to local conf directory zeppelin.conf.dir\nLinea 118:  WARN [2020-10-23 21:13:16,736] ({main} ZeppelinConfiguration.java[getConfigFSDir]:527) - zeppelin.config.fs.dir is not specified, fall back to local conf directory zeppelin.conf.dir\nLinea 119:  WARN [2020-10-23 21:13:17,150] ({main} NotebookAuthorization.java[getInstance]:86) - Notebook authorization module was called without initialization, initializing with default configuration\nLinea 120:  WARN [2020-11-08 18:51:57,079] ({pool-1-thread-1} ZeppelinConfiguration.java[create]:117) - Failed to load configuration, proceeding with a default\nLinea 121:  INFO [2020-10-28 19:38:17,798] ({main} ZeppelinConfiguration.java[create]:121) - Load configuration from file:/home/leo/bigdata/spark/zeppelin-0.8.2-bin-all/conf/zeppelin-site.xml\nLinea 122:  WARN [2020-10-28 19:38:19,342] ({main} ZeppelinConfiguration.java[getConfigFSDir]:527) - zeppelin.config.fs.dir is not specified, fall back to local conf directory zeppelin.conf.dir\nLinea 123:  WARN [2020-10-28 19:38:19,343] ({main} ZeppelinConfiguration.java[getConfigFSDir]:527) - zeppelin.config.fs.dir is not specified, fall back to local conf directory zeppelin.conf.dir\nLinea 124:  WARN [2020-10-28 19:38:20,191] ({main} NotebookAuthorization.java[getInstance]:86) - Notebook authorization module was called without initialization, initializing with default configuration\nLinea 125:  INFO [2020-10-21 19:18:40,435] ({main} ZeppelinConfiguration.java[create]:121) - Load configuration from file:/home/leo/bigdata/spark/zeppelin-0.8.2-bin-all/conf/zeppelin-site.xml\nLinea 126:  WARN [2020-10-21 19:18:41,811] ({main} ZeppelinConfiguration.java[getConfigFSDir]:527) - zeppelin.config.fs.dir is not specified, fall back to local conf directory zeppelin.conf.dir\nLinea 127:  WARN [2020-10-21 19:18:41,812] ({main} ZeppelinConfiguration.java[getConfigFSDir]:527) - zeppelin.config.fs.dir is not specified, fall back to local conf directory zeppelin.conf.dir\nLinea 128:  WARN [2020-10-21 19:18:42,288] ({main} NotebookAuthorization.java[getInstance]:86) - Notebook authorization module was called without initialization, initializing with default configuration\nLinea 129:  WARN [2020-10-23 21:11:40,210] ({pool-1-thread-1} ZeppelinConfiguration.java[create]:117) - Failed to load configuration, proceeding with a default\nLinea 130:  WARN [2020-10-23 21:13:35,036] ({pool-1-thread-1} ZeppelinConfiguration.java[create]:117) - Failed to load configuration, proceeding with a default\nLinea 131:  WARN [2020-10-28 19:40:41,171] ({pool-1-thread-3} ZeppelinConfiguration.java[create]:117) - Failed to load configuration, proceeding with a default\nLinea 132:  INFO [2020-10-28 20:52:03,457] ({Thread-17} HiveMetaStore.java[addAdminUsers_core]:712) - No user is added in admin role, since config is empty\nLinea 133:  WARN [2020-11-08 11:38:04,641] ({pool-1-thread-1} ZeppelinConfiguration.java[create]:117) - Failed to load configuration, proceeding with a default\nLinea 134:  INFO [2020-11-08 11:42:08,721] ({Thread-17} HiveMetaStore.java[addAdminUsers_core]:712) - No user is added in admin role, since config is empty\nLinea 135:  INFO [2020-11-08 12:04:45,409] ({Thread-17} TrashPolicyDefault.java[initialize]:92) - Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\nLinea 136:  INFO [2020-11-08 12:05:44,661] ({pool-2-thread-7} TrashPolicyDefault.java[initialize]:92) - Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\nLinea 137:  WARN [2020-11-08 18:51:26,862] ({pool-1-thread-1} ZeppelinConfiguration.java[create]:117) - Failed to load configuration, proceeding with a default\nLinea 138:  INFO [2020-11-08 18:51:52,394] ({Thread-17} HiveMetaStore.java[addAdminUsers_core]:712) - No user is added in admin role, since config is empty\nLinea 139:  WARN [2020-10-24 11:18:22,938] ({pool-1-thread-1} ZeppelinConfiguration.java[create]:117) - Failed to load configuration, proceeding with a default\n"}]},"apps":[],"jobName":"paragraph_1603545804769_274302879","id":"20201023-002107_2147167260","dateCreated":"2020-10-24T10:23:24-0300","dateStarted":"2020-11-08T19:47:11-0300","dateFinished":"2020-11-08T19:47:11-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2986"},{"text":"print(s\"\"\"%html\n<img src=\"$baseDir/02_spark_core/log_linage.png\" alt=\"Drawing\" style=\"width: 70%;\"/>\n\"\"\")\n","user":"anonymous","dateUpdated":"2020-11-08T19:47:11-0300","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","editorHide":true,"fontSize":9,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<img src=\"https://bitbucket.org/bigdata_famaf/diplodatos_bigdata/raw/HEAD/clases/02_spark_core/log_linage.png\" alt=\"Drawing\" style=\"width: 70%;\"/>\n"}]},"apps":[],"jobName":"paragraph_1603545804769_-5763461","id":"20171013-164802_1824704614","dateCreated":"2020-10-24T10:23:24-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2987"},{"text":"{\nval uiHost = sc.getConf.getOption(\"spark.driver.host\").getOrElse(\"localhost\")\nval uiPort = sc.getConf.getOption(\"spark.ui.port\").getOrElse(\"4040\")\n\nval textLocal = s\"\"\"\n%html\nEjecutar celda y ver en Spark UI tareas y grafo de operaciones\n<a href=\"http://$uiHost:$uiPort\">http://$uiHost(host):$uiPort(port)</a>\n\"\"\"\n\nval textNabuco = s\"\"\"\n%html\nEjecutar celda y ver en Spark UI tareas y grafo de operaciones\n<a href=\"http://localhost:4040\">http://localhost(host):4040(port)</a><br>\nRecordar hacer el tunel ssh:<br>\nssh -vCN -L 4040:localhost:$uiPort -l &lt;tu login&gt; nabucodonosor.ccad.unc.edu.ar\n\"\"\"\n\nif (uiHost == \"200.16.29.165\")\n    print(textNabuco)\nelse\n    print(textLocal)\n}\n","user":"anonymous","dateUpdated":"2020-11-08T19:47:11-0300","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"fontSize":14,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\n"},{"type":"HTML","data":"Ejecutar celda y ver en Spark UI tareas y grafo de operaciones\n<a href=\"http://192.168.0.108:4040\">http://192.168.0.108(host):4040(port)</a>\n"}]},"apps":[],"jobName":"paragraph_1603545804770_892683233","id":"20171013-163432_1466279672","dateCreated":"2020-10-24T10:23:24-0300","dateStarted":"2020-11-08T19:47:11-0300","dateFinished":"2020-11-08T19:47:12-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2988"},{"title":"Implementación","text":"%md\n\n* El RDD se distribuye en **particiones** en nodos del cluster (o fs local).\n* Se construye el **grafo de operaciones**.\n* Las operaciones de dividen en **tasks** (tareas).\n* A cada **partición** se le aplica una **task**.\n* Las tareas son ejecutadas por los executors en nodos (o threads locales).\n","user":"anonymous","dateUpdated":"2020-11-08T19:47:12-0300","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"title":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<ul>\n  <li>El RDD se distribuye en <strong>particiones</strong> en nodos del cluster (o fs local).</li>\n  <li>Se construye el <strong>grafo de operaciones</strong>.</li>\n  <li>Las operaciones de dividen en <strong>tasks</strong> (tareas).</li>\n  <li>A cada <strong>partición</strong> se le aplica una <strong>task</strong>.</li>\n  <li>Las tareas son ejecutadas por los executors en nodos (o threads locales).</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1603545804770_895387209","id":"20171013-123100_1037283294","dateCreated":"2020-10-24T10:23:24-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2989"},{"text":"%md\n","user":"anonymous","dateUpdated":"2020-11-08T19:47:12-0300","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1603549205495_-698528992","id":"20201024-112005_1584393800","dateCreated":"2020-10-24T11:20:05-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2990"},{"text":"%md\n","user":"anonymous","dateUpdated":"2020-11-08T19:47:12-0300","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"editorMode":"ace/mode/markdown"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1603549202636_1845658580","id":"20201024-112002_1463255077","dateCreated":"2020-10-24T11:20:02-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2991"},{"title":"Ejercicio","text":"%md\n\n* Cree una celda nueva y copie en ella el último programa sin las líneas 13 en adelante.\n* Observe en Spark UI las tareas ejecutadas.\n","user":"anonymous","dateUpdated":"2020-11-08T19:47:12-0300","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"title":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<ul>\n  <li>Cree una celda nueva y copie en ella el último programa sin las líneas 13 en adelante.</li>\n  <li>Observe en Spark UI las tareas ejecutadas.</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1603545804770_-431623926","id":"20171013-165833_179635135","dateCreated":"2020-10-24T10:23:24-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2992"},{"text":"%pyspark\ninputRDD = sc.textFile(\"./logs/\") # RDD de entrada\n\n# se crea un nuevo RDD:\nerrorRDD = inputRDD.filter(lambda line: \"ERROR\" in line) \n\n# se crea otro nuevo RDD\nconfigRDD = inputRDD.filter(lambda line: \"config\" in line) \n\nerrOrConfRDD = errorRDD.union(configRDD) ","user":"anonymous","dateUpdated":"2020-11-08T19:47:12-0300","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1603926666218_-216599852","id":"20201028-201106_581163772","dateCreated":"2020-10-28T20:11:06-0300","dateStarted":"2020-11-08T19:47:12-0300","dateFinished":"2020-11-08T19:47:12-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2993"},{"text":"%md\n## ~.- Evaluación Lazy\n\nEn Spark todas las **transformaciones** (`map`, `filter`, `union`, etc.) son evaluadas de forma **lazy**:\n\n* son acumuladas como *grafo de operaciones*\n* se ejecutan al momento de traer los datos al driver (`collect`, `take`, etc.)\n    - se llama a una **acción**.\n\nEsto permite:\n\n* hacer **optimizaciones**\n    - se computa solo lo que hace falta (tiene mucho sentido en Big Data)\n    - se hace un *pipeling* de transformaciones sin guardar resultados intermedios \n* recalcular las dependencias si hay algún fallo (resilencia)\n","user":"anonymous","dateUpdated":"2020-11-08T19:47:12-0300","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>~.- Evaluación Lazy</h2>\n<p>En Spark todas las <strong>transformaciones</strong> (<code>map</code>, <code>filter</code>, <code>union</code>, etc.) son evaluadas de forma <strong>lazy</strong>:</p>\n<ul>\n  <li>son acumuladas como <em>grafo de operaciones</em></li>\n  <li>se ejecutan al momento de traer los datos al driver (<code>collect</code>, <code>take</code>, etc.)\n    <ul>\n      <li>se llama a una <strong>acción</strong>.</li>\n    </ul>\n  </li>\n</ul>\n<p>Esto permite:</p>\n<ul>\n  <li>hacer <strong>optimizaciones</strong>\n    <ul>\n      <li>se computa solo lo que hace falta (tiene mucho sentido en Big Data)</li>\n      <li>se hace un <em>pipeling</em> de transformaciones sin guardar resultados intermedios</li>\n    </ul>\n  </li>\n  <li>recalcular las dependencias si hay algún fallo (resilencia)</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1603545804770_-202962699","id":"20171013-171238_638394270","dateCreated":"2020-10-24T10:23:24-0300","dateStarted":"2020-11-08T19:47:12-0300","dateFinished":"2020-11-08T19:47:12-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2994"},{"title":"Logs análisis (muestra solo 2 lineas)","text":"%pyspark\n\ninputRDD = sc.textFile(\"./logs/\") # RDD de entrada\nerrorRDD = inputRDD.filter(lambda line: \"ERROR\" in line) #  se crea un nuevo RDD\nconfigRDD = inputRDD.filter(lambda line: \"config\" in line) # se crea un nuevo RDD\n\nerrOrConfRDD = errorRDD.union(configRDD) \n\nfor ln, l in enumerate(errOrConfRDD.take(2)): # take(2) en vez de collect\n    print(\"Linea {}:\".format(ln), l)\n\n# Compara con primer programa en Spark UI\n","user":"anonymous","dateUpdated":"2020-11-08T19:47:12-0300","config":{"lineNumbers":true,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":14,"editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Linea 0:  WARN [2020-11-08 12:10:54,455] ({pool-2-thread-21} NotebookServer.java[afterStatusChange]:2316) - Job 20201108-120604_1359724646 is finished, status: ERROR, exception: null, result: %text Fail to execute line 1: flightsDF.groupBy(\"Origin\") \\\nLinea 1:  WARN [2020-11-08 12:14:27,366] ({pool-2-thread-22} NotebookServer.java[afterStatusChange]:2316) - Job 20201108-120604_1359724646 is finished, status: ERROR, exception: null, result: %text Traceback (most recent call last):\n"}]},"apps":[],"jobName":"paragraph_1603545804771_775748941","id":"20201023-002121_604229819","dateCreated":"2020-10-24T10:23:24-0300","dateStarted":"2020-11-08T19:47:12-0300","dateFinished":"2020-11-08T19:47:12-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2995"},{"title":"Ejercicio","text":"%md\n\nComplete los `...` en el siguiente programa para contar la cantidad de veces que aparece la letra 'c' en los archivos en `./logs/`.\n\n#### Ayuda\n\n\n* Se puede usar el método `.filter` (ya visto en ejemplos anteriores) para crear un RDD solo con la letra C.\n* El método `count` de RDD cuenta la cantidad de elementos.\n* La letra 'c' se escribe `'c'` en Scala.\n","user":"anonymous","dateUpdated":"2020-11-08T19:47:12-0300","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"title":true,"results":{},"enabled":true,"fontSize":9},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Complete los <code>...</code> en el siguiente programa para contar la cantidad de veces que aparece la letra &lsquo;c&rsquo; en los archivos en <code>./logs/</code>.</p>\n<h4>Ayuda</h4>\n<ul>\n  <li>Se puede usar el método <code>.filter</code> (ya visto en ejemplos anteriores) para crear un RDD solo con la letra C.</li>\n  <li>El método <code>count</code> de RDD cuenta la cantidad de elementos.</li>\n  <li>La letra &lsquo;c&rsquo; se escribe <code>&#39;c&#39;</code> en Scala.</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1603545804771_249096000","id":"20171013-174042_1672649057","dateCreated":"2020-10-24T10:23:24-0300","dateStarted":"2020-11-08T19:47:12-0300","dateFinished":"2020-11-08T19:47:12-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2996"},{"text":"%pyspark\n\nlinesRDD = sc.textFile(\"./logs/\")\n\ncharsRDD = linesRDD \\\n            .flatMap(lambda l: l)\n\nonlyCRDD = charsRDD \\\n            .filter(lambda car: car == 'c')\n\nprint(\"Aparecen {} letras c en los logs.\".format(onlyCRDD.count()))\n","user":"anonymous","dateUpdated":"2020-11-08T19:47:12-0300","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":14,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Aparecen 2137327 letras c en los logs.\n"}]},"apps":[],"jobName":"paragraph_1603545804771_-1109595129","id":"20171013-175507_696892344","dateCreated":"2020-10-24T10:23:24-0300","dateStarted":"2020-11-08T19:47:12-0300","dateFinished":"2020-11-08T19:47:15-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2997"},{"text":"%md\n## ~.- Persistencia\n\nSpark **recomputa** el grafo de dependencias cuando se llama una **acción**:","user":"anonymous","dateUpdated":"2020-11-08T19:47:15-0300","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>~.- Persistencia</h2>\n<p>Spark <strong>recomputa</strong> el grafo de dependencias cuando se llama una <strong>acción</strong>:</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1603545804771_-190919102","id":"20171016-174448_43219511","dateCreated":"2020-10-24T10:23:24-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:2998"},{"text":"%pyspark\n\ninput = sc.parallelize(range(30)) # Se crea la lista [0,...,29] y se lo convierte en RDD \n\nresult = input.map(lambda x: x*x)\n\nprint(\"La media es \", result.mean()) # computa los cuadrados\n\nfor r in result.collect():\n     print(r) # recomputa los cuadrados :(","user":"anonymous","dateUpdated":"2020-11-08T19:47:15-0300","config":{"lineNumbers":true,"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":14,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"La media es  285.1666666666667\n0\n1\n4\n9\n16\n25\n36\n49\n64\n81\n100\n121\n144\n169\n196\n225\n256\n289\n324\n361\n400\n441\n484\n529\n576\n625\n676\n729\n784\n841\n"}]},"apps":[],"jobName":"paragraph_1603545804771_-4662965","id":"20201023-002135_640378642","dateCreated":"2020-10-24T10:23:24-0300","dateStarted":"2020-11-08T19:47:15-0300","dateFinished":"2020-11-08T19:47:15-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2999"},{"text":"{\nval uiHost = sc.getConf.getOption(\"spark.driver.host\").getOrElse(\"localhost\")\nval uiPort = sc.getConf.getOption(\"spark.ui.port\").getOrElse(\"4040\")\n\nval textLocal = s\"\"\"\n%html\n(ver resultado en Spark UI\n<a href=\"http://$uiHost:$uiPort\">http://$uiHost(host):$uiPort(port)</a>)\n<br>\n<br>\nPara evitarlo Spark puede cachear los datos:\n\"\"\"\n\nval textNabuco = s\"\"\"\n%html\n(ver resultado en Spark UI\n<a href=\"http://localhost:4040\">http://localhost(host):4040(port)</a>\n<br>\n<br>\nPara evitarlo Spark puede cachear los datos:\n\"\"\"\n\nif (uiHost == \"200.16.29.165\")\n    print(textNabuco)\nelse\n    print(textLocal)\n}\n","user":"anonymous","dateUpdated":"2020-11-08T19:47:15-0300","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\n"},{"type":"HTML","data":"(ver resultado en Spark UI\n<a href=\"http://192.168.0.108:4040\">http://192.168.0.108(host):4040(port)</a>)\n<br>\n<br>\nPara evitarlo Spark puede cachear los datos:\n"}]},"apps":[],"jobName":"paragraph_1603545804771_471223601","id":"20171016-175252_2114983095","dateCreated":"2020-10-24T10:23:24-0300","dateStarted":"2020-11-08T19:47:15-0300","dateFinished":"2020-11-08T19:47:15-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3000"},{"text":"%pyspark\n\ninput = sc.parallelize(range(30)) # Se crea la lista [0,...,29] y se lo convierte en RDD \n\nresult = input.map(lambda x: x*x) \\\n              .setName(\"cuadrados\").cache() # cache de datos\n\nprint(\"La media es \", result.mean()) # computa los cuadrados\n\nfor r in result.collect():\n     print(r) # no recomputa el map :)","user":"anonymous","dateUpdated":"2020-11-08T19:47:15-0300","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/python","fontSize":14,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"La media es  285.1666666666667\n0\n1\n4\n9\n16\n25\n36\n49\n64\n81\n100\n121\n144\n169\n196\n225\n256\n289\n324\n361\n400\n441\n484\n529\n576\n625\n676\n729\n784\n841\n"}]},"apps":[],"jobName":"paragraph_1603545804772_-808719890","id":"20201023-002155_1881617494","dateCreated":"2020-10-24T10:23:24-0300","dateStarted":"2020-11-08T19:47:16-0300","dateFinished":"2020-11-08T19:47:16-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3001"},{"text":"{\nval uiHost = sc.getConf.getOption(\"spark.driver.host\").getOrElse(\"localhost\")\nval uiPort = sc.getConf.getOption(\"spark.ui.port\").getOrElse(\"4040\")\n\nval textLocal = s\"\"\"\n%html\nVer resultado en Spark UI\n<a href=\"http://$uiHost:$uiPort/storage\">http://$uiHost(host):$uiPort(port)/storage</a>\n<br>\nObservar tambien green dots en Dag Visualization.\n\"\"\"\n\nval textNabuco = s\"\"\"\n%html\nVer resultado en Spark UI\n<a href=\"http://localhost:4040/storage\">http://localhost(host):4040(port)/storage</a>\n<br>\nObservar tambien green dots en Dag Visualization.\n\"\"\"\n\nif (uiHost == \"200.16.29.165\")\n    print(textNabuco)\nelse\n    print(textLocal)\n}\n","user":"anonymous","dateUpdated":"2020-11-08T19:47:16-0300","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","editorHide":true,"fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\n"},{"type":"HTML","data":"Ver resultado en Spark UI\n<a href=\"http://192.168.0.108:4040/storage\">http://192.168.0.108(host):4040(port)/storage</a>\n<br>\nObservar tambien green dots en Dag Visualization.\n"}]},"apps":[],"jobName":"paragraph_1603545804772_-652229334","id":"20171016-180034_191267646","dateCreated":"2020-10-24T10:23:24-0300","dateStarted":"2020-11-08T19:47:16-0300","dateFinished":"2020-11-08T19:47:16-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3002"},{"text":"%md\n\n## ~.- Implementación API Python\n","user":"anonymous","dateUpdated":"2020-11-08T19:47:16-0300","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<h2>~.- Implementación API Python</h2>\n</div>"}]},"apps":[],"jobName":"paragraph_1603545804772_662429322","id":"20191128-173809_1077146591","dateCreated":"2020-10-24T10:23:24-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3003"},{"text":"%md\n\n* Spark esta originalmente implementado en Scala/Java.\n* `SparkContext` de Python usa [Py4J](https://www.py4j.org/), lanza JVM local y crea `JavaSparkContext`.\n* [Py4J](https://www.py4j.org/) solo se usa en driver.\n* En máquinas remotas los executors corren en JVM asegurando resilencia.\n* Estas JVM lanzan procesos Python.\n* [Mas info](https://medium.com/@ketanvatsalya/a-scenic-route-through-pyspark-internals-feaf74ed660d).\n","user":"anonymous","dateUpdated":"2020-11-08T19:47:16-0300","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionKey":"TAB","completionSupport":false},"colWidth":6,"editorMode":"ace/mode/markdown","fontSize":9,"editorHide":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<ul>\n  <li>Spark esta originalmente implementado en Scala/Java.</li>\n  <li><code>SparkContext</code> de Python usa <a href=\"https://www.py4j.org/\">Py4J</a>, lanza JVM local y crea <code>JavaSparkContext</code>.</li>\n  <li><a href=\"https://www.py4j.org/\">Py4J</a> solo se usa en driver.</li>\n  <li>En máquinas remotas los executors corren en JVM asegurando resilencia.</li>\n  <li>Estas JVM lanzan procesos Python.</li>\n  <li><a href=\"https://medium.com/@ketanvatsalya/a-scenic-route-through-pyspark-internals-feaf74ed660d\">Mas info</a>.</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1603545804772_-1548360349","id":"20191128-175341_672335059","dateCreated":"2020-10-24T10:23:24-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3004"},{"text":"print(s\"\"\"%html\n<img src=\"$baseDir/02_spark_core/python-spark.png\" alt=\"Drawing\" style=\"width: 70%;\"/>\n\"\"\")\n","user":"anonymous","dateUpdated":"2020-11-08T19:47:16-0300","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":6,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<img src=\"https://bitbucket.org/bigdata_famaf/diplodatos_bigdata/raw/HEAD/clases/02_spark_core/python-spark.png\" alt=\"Drawing\" style=\"width: 70%;\"/>\n"}]},"apps":[],"jobName":"paragraph_1603545804773_-1555132642","id":"20191128-175407_1271062039","dateCreated":"2020-10-24T10:23:24-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3005"},{"title":"Ejercicio","text":"%md\nComplete el siguiente programa para que cuente la cantidad de lineas que comienzan con la palabra `INFO`, `WARN` y `ERROR`.\n\nTambién, haga cache de los RDD para hacer el programa más eficiente. \n","user":"anonymous","dateUpdated":"2020-11-08T19:47:16-0300","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"title":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Complete el siguiente programa para que cuente la cantidad de lineas que comienzan con la palabra <code>INFO</code>, <code>WARN</code> y <code>ERROR</code>.</p>\n<p>También, haga cache de los RDD para hacer el programa más eficiente.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1603545804773_-1000361310","id":"20171016-193030_671507369","dateCreated":"2020-10-24T10:23:24-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3006"},{"text":"%pyspark\n\nlinesRDD = sc.textFile(\"./logs/\") # RDD de entrada\n\nlinesStrip = linesRDD.map(lambda l: l.strip()).cache() # Borro espacios en borde\n\nlinesInfo = linesStrip.filter(lambda l: l.startswith(\"INFO\"))\n\nlinesWarn = linesStrip.filter(lambda l: l.startswith(\"WARN\"))\n\nlinesError = linesStrip.filter(lambda l: l.startswith(\"ERROR\"))\n\nprint(\"Cantidad de lineas INFO: \", linesInfo.count())\n\nprint(\"Cantidad de lineas WARN: \", linesWarn.count())\n\nprint(\"Cantidad de lineas ERROR: \",linesError .count())\n","user":"anonymous","dateUpdated":"2020-11-08T19:47:16-0300","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":14,"editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Cantidad de lineas INFO:  580538\nCantidad de lineas WARN:  981\nCantidad de lineas ERROR:  20\n"}]},"apps":[],"jobName":"paragraph_1603545804773_1940699965","id":"20191123-214023_2104486544","dateCreated":"2020-10-24T10:23:24-0300","dateStarted":"2020-11-08T19:47:16-0300","dateFinished":"2020-11-08T19:47:17-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3007"},{"title":"Ejercicio","text":"%md\nEl archivo en `~/diplodatos_bigdata/ds/flights.csv` contiene información de vuelos realizados en 2008 (solo 100.000), uno por línea.\n\nLos datos estan separados por coma y la columna `Cancelled` (la 22) tiene un `1` si el vuelo fue cancelado. Además si el vuelo fue redirigido se indica con '1' en la columna `Diverted` (la 24).\n\nCompletar el siguiente programa que devuelve el porcentaje de vuelos cancelados y el porcentaje de redirigidos.\n\nUtilizar cache si lo cree conveniente.\n","user":"anonymous","dateUpdated":"2020-11-08T19:47:17-0300","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false,"completionKey":"TAB"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"title":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>El archivo en <code>~/diplodatos_bigdata/ds/flights.csv</code> contiene información de vuelos realizados en 2008 (solo 100.000), uno por línea.</p>\n<p>Los datos estan separados por coma y la columna <code>Cancelled</code> (la 22) tiene un <code>1</code> si el vuelo fue cancelado. Además si el vuelo fue redirigido se indica con &lsquo;1&rsquo; en la columna <code>Diverted</code> (la 24).</p>\n<p>Completar el siguiente programa que devuelve el porcentaje de vuelos cancelados y el porcentaje de redirigidos.</p>\n<p>Utilizar cache si lo cree conveniente.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1603545804773_-2146075029","id":"20171016-224717_280061616","dateCreated":"2020-10-24T10:23:24-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3008"},{"text":"{val input = spark.read.format(\"csv\").option(\"header\", \"true\").load(s\"${homeDir}/diplodatos_bigdata/ds/flights.csv\").sample(false,0.0005)\nz.show(input,10)}","user":"anonymous","dateUpdated":"2020-11-08T19:47:17-0300","config":{"tableHide":false,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"editorHide":false,"results":{"0":{"graph":{"mode":"table","height":300,"optionOpen":false,"setting":{"table":{"tableGridState":{"columns":[{"name":"Year","visible":true,"width":"*","sort":{"priority":0,"direction":"asc"},"filters":[{}],"pinned":""},{"name":"Month","visible":true,"width":"*","sort":{},"filters":[{}],"pinned":""},{"name":"DayofMonth","visible":true,"width":"*","sort":{},"filters":[{}],"pinned":""},{"name":"DayOfWeek","visible":true,"width":"*","sort":{},"filters":[{}],"pinned":""},{"name":"DepTime","visible":true,"width":"*","sort":{},"filters":[{}],"pinned":""},{"name":"CRSDepTime","visible":true,"width":"*","sort":{},"filters":[{}],"pinned":""},{"name":"ArrTime","visible":true,"width":"*","sort":{},"filters":[{}],"pinned":""},{"name":"CRSArrTime","visible":true,"width":"*","sort":{},"filters":[{}],"pinned":""},{"name":"UniqueCarrier","visible":true,"width":"*","sort":{},"filters":[{}],"pinned":""},{"name":"FlightNum","visible":true,"width":"*","sort":{},"filters":[{}],"pinned":""},{"name":"TailNum","visible":true,"width":"*","sort":{},"filters":[{}],"pinned":""},{"name":"ActualElapsedTime","visible":true,"width":"*","sort":{},"filters":[{}],"pinned":""},{"name":"CRSElapsedTime","visible":true,"width":"*","sort":{},"filters":[{}],"pinned":""},{"name":"AirTime","visible":true,"width":"*","sort":{},"filters":[{}],"pinned":""},{"name":"ArrDelay","visible":true,"width":"*","sort":{},"filters":[{}],"pinned":""},{"name":"DepDelay","visible":true,"width":"*","sort":{},"filters":[{}],"pinned":""},{"name":"Origin","visible":true,"width":"*","sort":{},"filters":[{}],"pinned":""},{"name":"Dest","visible":true,"width":"*","sort":{},"filters":[{}],"pinned":""},{"name":"Distance","visible":true,"width":"*","sort":{},"filters":[{}],"pinned":""},{"name":"TaxiIn","visible":true,"width":"*","sort":{},"filters":[{}],"pinned":""},{"name":"TaxiOut","visible":true,"width":"*","sort":{},"filters":[{}],"pinned":""},{"name":"Cancelled","visible":true,"width":"*","sort":{},"filters":[{}],"pinned":""},{"name":"CancellationCode","visible":true,"width":"*","sort":{},"filters":[{}],"pinned":""},{"name":"Diverted","visible":true,"width":"*","sort":{},"filters":[{}],"pinned":""},{"name":"CarrierDelay","visible":true,"width":"*","sort":{},"filters":[{}],"pinned":""},{"name":"WeatherDelay","visible":true,"width":"*","sort":{},"filters":[{}],"pinned":""},{"name":"NASDelay","visible":true,"width":"*","sort":{},"filters":[{}],"pinned":""},{"name":"SecurityDelay","visible":true,"width":"*","sort":{},"filters":[{}],"pinned":""},{"name":"LateAircraftDelay","visible":true,"width":"*","sort":{},"filters":[{}],"pinned":""}],"scrollFocus":{},"selection":[],"grouping":{"grouping":[],"aggregations":[],"rowExpandedStates":{}},"treeView":{},"pagination":{"paginationCurrentPage":1,"paginationPageSize":250}},"tableColumnTypeState":{"names":{"Year":"string","Month":"string","DayofMonth":"string","DayOfWeek":"string","DepTime":"string","CRSDepTime":"string","ArrTime":"string","CRSArrTime":"string","UniqueCarrier":"string","FlightNum":"string","TailNum":"string","ActualElapsedTime":"string","CRSElapsedTime":"string","AirTime":"string","ArrDelay":"string","DepDelay":"string","Origin":"string","Dest":"string","Distance":"string","TaxiIn":"string","TaxiOut":"string","Cancelled":"string","CancellationCode":"string","Diverted":"string","CarrierDelay":"string","WeatherDelay":"string","NASDelay":"string","SecurityDelay":"string","LateAircraftDelay":"string"},"updated":false},"tableOptionSpecHash":"[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]","tableOptionValue":{"useFilter":false,"showPagination":false,"showAggregationFooter":false},"updated":false,"initialized":false}},"commonSetting":{}},"helium":{}}},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TABLE","data":"Year\tMonth\tDayofMonth\tDayOfWeek\tDepTime\tCRSDepTime\tArrTime\tCRSArrTime\tUniqueCarrier\tFlightNum\tTailNum\tActualElapsedTime\tCRSElapsedTime\tAirTime\tArrDelay\tDepDelay\tOrigin\tDest\tDistance\tTaxiIn\tTaxiOut\tCancelled\tCancellationCode\tDiverted\tCarrierDelay\tWeatherDelay\tNASDelay\tSecurityDelay\tLateAircraftDelay\n2008\t1\t6\t7\t1801\t1805\t1941\t1955\tWN\t172\tN767SW\t160\t170\t146\t-14\t-4\tMCO\tMDW\t989\t4\t10\t0\tnull\t0\tNA\tNA\tNA\tNA\tNA\n2008\t1\t6\t7\t1023\t1010\t1136\t1135\tWN\t752\tN348SW\t73\t85\t63\t1\t13\tOAK\tLAS\t407\t4\t6\t0\tnull\t0\tNA\tNA\tNA\tNA\tNA\n2008\t1\t6\t7\t838\t830\t955\t945\tWN\t304\tN342SW\t77\t75\t56\t10\t8\tOAK\tLAX\t337\t5\t16\t0\tnull\t0\tNA\tNA\tNA\tNA\tNA\n2008\t1\t6\t7\t745\t745\t929\t930\tWN\t1586\tN318SW\t164\t165\t149\t-1\t0\tSTL\tABQ\t934\t4\t11\t0\tnull\t0\tNA\tNA\tNA\tNA\tNA\n2008\t1\t7\t1\t1340\t1340\t1440\t1440\tWN\t35\tN519SW\t60\t60\t47\t0\t0\tLBB\tDAL\t293\t3\t10\t0\tnull\t0\tNA\tNA\tNA\tNA\tNA\n2008\t1\t8\t2\t1850\t1850\t2041\t2045\tWN\t102\tN331SW\t231\t235\t219\t-4\t0\tMCI\tSMF\t1442\t5\t7\t0\tnull\t0\tNA\tNA\tNA\tNA\tNA\n2008\t1\t9\t3\t1834\t1835\t1943\t1940\tWN\t55\tN502SW\t69\t65\t54\t3\t-1\tAMA\tDAL\t324\t5\t10\t0\tnull\t0\tNA\tNA\tNA\tNA\tNA\n2008\t1\t9\t3\t923\t925\t1046\t1055\tWN\t1188\tN200WN\t83\t90\t72\t-9\t-2\tHOU\tBHM\t570\t2\t9\t0\tnull\t0\tNA\tNA\tNA\tNA\tNA\n2008\t1\t10\t4\t1600\t1600\t2337\t2350\tWN\t416\tN738CB\t277\t290\t254\t-13\t0\tLAS\tPVD\t2363\t4\t19\t0\tnull\t0\tNA\tNA\tNA\tNA\tNA\n2008\t1\t10\t4\t1154\t1155\t1314\t1310\tWN\t876\tN427WN\t80\t75\t68\t4\t-1\tONT\tSMF\t389\t4\t8\t0\tnull\t0\tNA\tNA\tNA\tNA\tNA\n2008\t1\t12\t6\t855\t825\t955\t930\tWN\t171\tN611SW\t60\t65\t49\t25\t30\tBUF\tBWI\t281\t4\t7\t0\tnull\t0\t25\t0\t0\t0\t0\n2008\t1\t12\t6\t900\t755\t957\t905\tWN\t2724\tN788SA\t57\t70\t46\t52\t65\tPIT\tPHL\t267\t4\t7\t0\tnull\t0\t52\t0\t0\t0\t0\n2008\t1\t13\t7\t1924\t1835\t2049\t2000\tWN\t2133\tN506SW\t85\t85\t49\t49\t49\tPHL\tPIT\t267\t6\t30\t0\tnull\t0\t8\t0\t0\t0\t41\n2008\t1\t13\t7\t1727\t1705\t1735\t1725\tWN\t3586\tN221WN\t68\t80\t54\t10\t22\tPHX\tLAX\t370\t7\t7\t0\tnull\t0\tNA\tNA\tNA\tNA\tNA\n2008\t1\t13\t7\tNA\t635\tNA\t805\tWN\t3512\tnull\tNA\t90\tNA\tNA\tNA\tSAN\tSFO\t447\tNA\tNA\t1\tB\t0\tNA\tNA\tNA\tNA\tNA\n2008\t1\t14\t1\t637\t635\t748\t745\tWN\t3138\tN355SW\t71\t70\t59\t3\t2\tCMH\tBWI\t336\t4\t8\t0\tnull\t0\tNA\tNA\tNA\tNA\tNA\n2008\t1\t15\t2\t2008\t2000\t2100\t2055\tWN\t56\tN502SW\t52\t55\t39\t5\t8\tHOU\tDAL\t239\t4\t9\t0\tnull\t0\tNA\tNA\tNA\tNA\tNA\n2008\t1\t15\t2\t1658\t1700\t1815\t1830\tWN\t1435\tN683SW\t137\t150\t123\t-15\t-2\tSAT\tPHX\t843\t6\t8\t0\tnull\t0\tNA\tNA\tNA\tNA\tNA\n2008\t1\t16\t3\t1432\t1435\t1759\t1815\tWN\t2205\tN235WN\t147\t160\t131\t-16\t-3\tABQ\tMDW\t1121\t8\t8\t0\tnull\t0\tNA\tNA\tNA\tNA\tNA\n2008\t1\t16\t3\t1302\t1305\t1427\t1430\tWN\t1541\tN228WN\t85\t85\t74\t-3\t-3\tOAK\tLAS\t407\t4\t7\t0\tnull\t0\tNA\tNA\tNA\tNA\tNA\n2008\t1\t17\t4\t1720\t1705\t1848\t1835\tWN\t369\tN735SA\t88\t90\t69\t13\t15\tSAN\tSFO\t447\t6\t13\t0\tnull\t0\tNA\tNA\tNA\tNA\tNA\n2008\t1\t20\t7\t1437\t1435\t2013\t2025\tWN\t3576\tN286WN\t216\t230\t203\t-12\t2\tLAX\tMDW\t1750\t6\t7\t0\tnull\t0\tNA\tNA\tNA\tNA\tNA\n2008\t1\t20\t7\t1101\t1100\t1404\t1440\tWN\t1047\tN495WN\t303\t340\t290\t-36\t1\tPHL\tPHX\t2075\t4\t9\t0\tnull\t0\tNA\tNA\tNA\tNA\tNA\n2008\t1\t23\t3\t725\t725\t1047\t1050\tWN\t3597\tN705SW\t202\t205\t188\t-3\t0\tPVD\tFLL\t1188\t4\t10\t0\tnull\t0\tNA\tNA\tNA\tNA\tNA\n2008\t1\t26\t6\t1511\t1445\t1740\t1735\tWN\t802\tN661SW\t89\t110\t74\t5\t26\tLAS\tBOI\t520\t3\t12\t0\tnull\t0\tNA\tNA\tNA\tNA\tNA\n2008\t1\t26\t6\t2017\t2015\t2112\t2115\tWN\t744\tN660SW\t55\t60\t42\t-3\t2\tSTL\tMDW\t251\t5\t8\t0\tnull\t0\tNA\tNA\tNA\tNA\tNA\n2008\t1\t26\t6\t630\t630\t723\t720\tWN\t828\tN271WN\t113\t110\t102\t3\t0\tTPA\tBNA\t612\t4\t7\t0\tnull\t0\tNA\tNA\tNA\tNA\tNA\n2008\t1\t30\t3\t639\t635\t748\t745\tWN\t3138\tN365SW\t69\t70\t52\t3\t4\tCMH\tBWI\t336\t2\t15\t0\tnull\t0\tNA\tNA\tNA\tNA\tNA\n2008\t1\t30\t3\t1549\t1550\t1642\t1655\tWN\t1595\tN212WN\t53\t65\t40\t-13\t-1\tLAS\tSAN\t258\t3\t10\t0\tnull\t0\tNA\tNA\tNA\tNA\tNA\n2008\t1\t31\t4\t735\t730\t914\t905\tWN\t1479\tN301SW\t159\t155\t148\t9\t5\tABQ\tOAK\t889\t3\t8\t0\tnull\t0\tNA\tNA\tNA\tNA\tNA\n2008\t1\t13\t7\t1846\t1825\t1948\t1938\tXE\t149\tN19554\t62\t73\t50\t10\t21\tMRY\tLGB\t283\t4\t8\t0\tnull\t0\tNA\tNA\tNA\tNA\tNA\n2008\t1\t2\t3\t1435\t1435\t1524\t1537\tXE\t502\tN16178\t109\t122\t90\t-13\t0\tSAT\tABQ\t609\t5\t14\t0\tnull\t0\tNA\tNA\tNA\tNA\tNA\n2008\t1\t1\t2\t1653\t1525\t1738\t1617\tXE\t600\tN12563\t105\t112\t91\t81\t88\tJAX\tMSY\t513\t6\t8\t0\tnull\t0\t0\t0\t0\t0\t81\n"},{"type":"TEXT","data":""}]},"apps":[],"jobName":"paragraph_1603545804774_-1903296947","id":"20191123-214248_1978352340","dateCreated":"2020-10-24T10:23:24-0300","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:3009"},{"title":"","text":"%pyspark\n\ninput = sc.textFile(\"../../diplodatos_bigdata/ds/flights.csv\")\n\nnTotal = input.count() - 1 # la primer fila tiene el nombre de las columnas\n\nparsed = input.map(lambda l: l.split(\",\")).cache()\n\n\ncancel = parsed.filter(lambda l: l[21] == '1')\n\nredir = parsed.filter(lambda l: l[23] == '1')\n\nnCancel = cancel.count()\nnRedir = redir.count()\n\nprint(\"total={}\".format(nTotal))\nprint(\"cancelados = {}%\".format(float(nCancel) * 100 / nTotal))\nprint(\"redireccionados = {}%\".format(float(nRedir) * 100 / nTotal))\n","user":"anonymous","dateUpdated":"2020-11-08T19:47:17-0300","config":{"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/python","fontSize":14,"editorHide":false,"title":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"total=100000\ncancelados = 1.142%\nredireccionados = 0.16%\n"}]},"apps":[],"jobName":"paragraph_1603545804774_1895481179","id":"20191124-133441_1910745321","dateCreated":"2020-10-24T10:23:24-0300","dateStarted":"2020-11-08T19:47:17-0300","dateFinished":"2020-11-08T19:47:17-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3010"},{"title":"Ejercicio","text":"%md\nLa columna 14 del mismo archivo tiene el tiempo del vuelo en minutos. Calcular el máximo.\n\n#### Ayuda\n\n* Busque en la documentacion de la [API RDD](http://spark.apache.org/docs/2.2.1/api/python/pyspark.html#pyspark.RDD) una acción para calcular el máximo.\n* Ojo que puede haber valores no definidos.\n","user":"anonymous","dateUpdated":"2020-11-08T19:47:17-0300","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true,"completionSupport":false},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>La columna 14 del mismo archivo tiene el tiempo del vuelo en minutos. Calcular el máximo.</p>\n<h4>Ayuda</h4>\n<ul>\n  <li>Busque en la documentacion de la <a href=\"http://spark.apache.org/docs/2.2.1/api/python/pyspark.html#pyspark.RDD\">API RDD</a> una acción para calcular el máximo.</li>\n  <li>Ojo que puede haber valores no definidos.</li>\n</ul>\n</div>"}]},"apps":[],"jobName":"paragraph_1603545804774_-1232178870","id":"20171016-232257_285172371","dateCreated":"2020-10-24T10:23:24-0300","dateStarted":"2020-11-08T19:47:17-0300","dateFinished":"2020-11-08T19:47:17-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3011"},{"text":"%pyspark\ninput = sc.textFile(\"../../diplodatos_bigdata/ds/flights.csv\")\nparsed = input.map(lambda l: l.split(\",\")).cache()\n\nmax_time = parsed.filter(lambda l: l[13] not in ['AirTime', 'NA']).map(lambda l: float(l[13])).max()\n\n\nprint(\"maximo tiempo de vuelo:{}\".format(max_time))\n\n\n\n","user":"anonymous","dateUpdated":"2020-11-08T19:47:17-0300","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"maximo tiempo de vuelo:369.0\n"}]},"apps":[],"jobName":"paragraph_1603928249989_1366461229","id":"20201028-203729_875715534","dateCreated":"2020-10-28T20:37:29-0300","dateStarted":"2020-11-08T19:47:18-0300","dateFinished":"2020-11-08T19:47:18-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3012"},{"title":"FIN","text":"//val baseDir=\"https://git.cs.famaf.unc.edu.ar/dbarsotti/diplodatos_bigdata/raw/master/clases\"\nval baseDir=\"https://bitbucket.org/bigdata_famaf/diplodatos_bigdata/raw/HEAD/clases\"\nval homeDir=\"/home/damian\"\n\nz.put(\"baseDir\", baseDir)\nprint(\"\"\"%html\n<script>\n    var heads = document.getElementsByTagName('h2');\n    var numHeads = heads.length;\n    var inner = \"\";\n    var i = 1;\n    var j = 0;\n    while (i < numHeads){\n        inner = heads[i].innerHTML;\n        if (inner.match(\"(~|\\d+)\\.-\") != -1 ) {\n            console.log(inner)\n            j++;\n            heads[i].innerHTML = inner.replace(/(~|\\d+)\\.-/,\"\"+j+\".-\");\n        }\n        i++\n    }\n</script>\n\"\"\")","user":"anonymous","dateUpdated":"2020-11-08T19:47:18-0300","config":{"tableHide":true,"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":true,"fontSize":9,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<script>\n    var heads = document.getElementsByTagName('h2');\n    var numHeads = heads.length;\n    var inner = \"\";\n    var i = 1;\n    var j = 0;\n    while (i < numHeads){\n        inner = heads[i].innerHTML;\n        if (inner.match(\"(~|\\d+)\\.-\") != -1 ) {\n            console.log(inner)\n            j++;\n            heads[i].innerHTML = inner.replace(/(~|\\d+)\\.-/,\"\"+j+\".-\");\n        }\n        i++\n    }\n</script>\nbaseDir: String = https://bitbucket.org/bigdata_famaf/diplodatos_bigdata/raw/HEAD/clases\nhomeDir: String = /home/damian\n"}]},"apps":[],"jobName":"paragraph_1603545804774_-581923402","id":"20171010-191336_1667301043","dateCreated":"2020-10-24T10:23:24-0300","dateStarted":"2020-11-08T19:47:18-0300","dateFinished":"2020-11-08T19:47:18-0300","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:3013"},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1604875638413_516208089","id":"20201108-194718_81697736","dateCreated":"2020-11-08T19:47:18-0300","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:3014"}],"name":"Diplodatos/Clase 02 - Spark Core","id":"2FN4M6VVD","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}